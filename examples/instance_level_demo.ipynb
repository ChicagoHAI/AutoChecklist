{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autochecklist Demo\n",
    "\n",
    "This notebook demonstrates the checklist generation and scoring methods implemented in `autochecklist`.\n",
    "\n",
    "**Methods covered:**\n",
    "1. **TICK pipeline** (DirectGenerator) + ChecklistScorer (batch mode) - Simple checklist from input\n",
    "2. **RLCF pipelines** (DirectGenerator/ContrastiveGenerator) + ChecklistScorer (weighted mode) - Weighted items with importance scores\n",
    "3. **RocketEval pipeline** (DirectGenerator) + ChecklistScorer (logprobs mode) - Confidence-aware scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:21:07.246498Z",
     "iopub.status.busy": "2026-02-27T06:21:07.246356Z",
     "iopub.status.idle": "2026-02-27T06:21:07.253840Z",
     "shell.execute_reply": "2026-02-27T06:21:07.252795Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# OpenRouter is the default provider. For other providers, see pipeline_demo.ipynb\n",
    "# Supported: OpenRouter, OpenAI (direct), vLLM (server or offline)\n",
    "assert os.getenv(\"OPENROUTER_API_KEY\"), \"Please set OPENROUTER_API_KEY in .env file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:21:07.255813Z",
     "iopub.status.busy": "2026-02-27T06:21:07.255678Z",
     "iopub.status.idle": "2026-02-27T06:21:07.370876Z",
     "shell.execute_reply": "2026-02-27T06:21:07.369961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance-level generators: DirectGenerator (tick, rocketeval, rlcf_direct), ContrastiveGenerator (rlcf_candidate)\n",
      "ChecklistScorer modes: batch, item (with capture_reasoning, use_logprobs, primary_metric options)\n"
     ]
    }
   ],
   "source": [
    "# Import the package\n",
    "from autochecklist import (\n",
    "    # Generators\n",
    "    DirectGenerator, ContrastiveGenerator,\n",
    "    # Scorer\n",
    "    ChecklistScorer,\n",
    "    # Models\n",
    "    Checklist, Score, ConfidenceLevel,\n",
    "    # Pipeline (recommended way)\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "print(\"Instance-level generators: DirectGenerator (tick, rocketeval, rlcf_direct), ContrastiveGenerator (rlcf_candidate)\")\n",
    "print(\"ChecklistScorer modes: batch, item (with capture_reasoning, use_logprobs, primary_metric options)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:21:07.427112Z",
     "iopub.status.busy": "2026-02-27T06:21:07.426757Z",
     "iopub.status.idle": "2026-02-27T06:21:07.430267Z",
     "shell.execute_reply": "2026-02-27T06:21:07.429369Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL = \"openai/gpt-5-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. TICK pipeline + ChecklistScorer (batch mode)\n",
    "\n",
    "**TICK pipeline** generates a checklist from just an input using few-shot prompting via `DirectGenerator`.\n",
    "\n",
    "**ChecklistScorer (batch mode)** evaluates all items in a single LLM call and returns the pass rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:21:07.432558Z",
     "iopub.status.busy": "2026-02-27T06:21:07.432367Z",
     "iopub.status.idle": "2026-02-27T06:21:20.019585Z",
     "shell.execute_reply": "2026-02-27T06:21:20.018851Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Write a haiku about autumn leaves falling.\n",
      "\n",
      "Generated 4 checklist items:\n",
      "1. Is the response written as a haiku (presented in three lines)?\n",
      "2. Does the haiku follow the traditional 5-7-5 syllable pattern?\n",
      "3. Is the subject of the haiku clearly about autumn leaves falling?\n",
      "4. Does the response contain only the haiku (no additional unrelated explanation or text)?\n"
     ]
    }
   ],
   "source": [
    "# Initialize TICK generator via pipeline preset\n",
    "tick = DirectGenerator(method_name=\"tick\", model=MODEL)\n",
    "\n",
    "# Generate checklist from input\n",
    "input_text = \"Write a haiku about autumn leaves falling.\"\n",
    "checklist = tick.generate(input=input_text)\n",
    "\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"\\nGenerated {len(checklist.items)} checklist items:\")\n",
    "print(checklist.to_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:21:20.021384Z",
     "iopub.status.busy": "2026-02-27T06:21:20.021242Z",
     "iopub.status.idle": "2026-02-27T06:21:30.511007Z",
     "shell.execute_reply": "2026-02-27T06:21:30.509468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOD Response:\n",
      "Crimson leaves drift down\n",
      "Dancing on the autumn breeze\n",
      "Nature's last ballet\n",
      "\n",
      "Pass rate: 100%\n",
      "\n",
      "Item scores:\n",
      "  [ChecklistItemAnswer.YES] Is the response written as a haiku (presented in three lines)?\n",
      "  [ChecklistItemAnswer.YES] Does the haiku follow the traditional 5-7-5 syllable pattern?\n",
      "  [ChecklistItemAnswer.YES] Is the subject of the haiku clearly about autumn leaves falling?\n",
      "  [ChecklistItemAnswer.YES] Does the response contain only the haiku (no additional unrelated explanation or text)?\n"
     ]
    }
   ],
   "source": [
    "# Score a GOOD response\n",
    "scorer = ChecklistScorer(mode=\"batch\", model=MODEL)\n",
    "\n",
    "good_response = \"\"\"Crimson leaves drift down\n",
    "Dancing on the autumn breeze\n",
    "Nature's last ballet\"\"\"\n",
    "\n",
    "score = scorer.score(checklist, target=good_response, input=input_text)\n",
    "\n",
    "print(\"GOOD Response:\")\n",
    "print(good_response)\n",
    "print(f\"\\nPass rate: {score.pass_rate:.0%}\")\n",
    "print(\"\\nItem scores:\")\n",
    "for item, item_score in zip(checklist.items, score.item_scores):\n",
    "    print(f\"  [{item_score.answer}] {item.question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:21:30.514214Z",
     "iopub.status.busy": "2026-02-27T06:21:30.513904Z",
     "iopub.status.idle": "2026-02-27T06:21:41.797817Z",
     "shell.execute_reply": "2026-02-27T06:21:41.796336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAD Response:\n",
      "Autumn is nice. I like the colors. The weather is cool.\n",
      "\n",
      "Pass rate: 0%\n",
      "\n",
      "Item scores:\n",
      "  [ChecklistItemAnswer.NO] Is the response written as a haiku (presented in three lines)?\n",
      "  [ChecklistItemAnswer.NO] Does the haiku follow the traditional 5-7-5 syllable pattern?\n",
      "  [ChecklistItemAnswer.NO] Is the subject of the haiku clearly about autumn leaves falling?\n",
      "  [ChecklistItemAnswer.NO] Does the response contain only the haiku (no additional unrelated explanation or text)?\n"
     ]
    }
   ],
   "source": [
    "# Score a BAD response\n",
    "bad_response = \"Autumn is nice. I like the colors. The weather is cool.\"\n",
    "\n",
    "score = scorer.score(checklist, target=bad_response, input=input_text)\n",
    "\n",
    "print(\"BAD Response:\")\n",
    "print(bad_response)\n",
    "print(f\"\\nPass rate: {score.pass_rate:.0%}\")\n",
    "print(\"\\nItem scores:\")\n",
    "for item, item_score in zip(checklist.items, score.item_scores):\n",
    "    print(f\"  [{item_score.answer}] {item.question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChecklistScorer (item mode with reasoning)\n",
    "\n",
    "**ChecklistScorer with `mode=\"item\"` and `capture_reasoning=True`** evaluates each checklist item individually (one LLM call per question).\n",
    "\n",
    "This mirrors the original TICK paper's evaluation methodology. Useful when:\n",
    "- You want more accurate per-item judgments\n",
    "- You want individual analysis reasoning for each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:21:41.801094Z",
     "iopub.status.busy": "2026-02-27T06:21:41.800791Z",
     "iopub.status.idle": "2026-02-27T06:22:14.076845Z",
     "shell.execute_reply": "2026-02-27T06:22:14.075504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item-mode scorer results:\n",
      "  Pass rate: 100%\n",
      "  LLM calls made: 4\n",
      "\n",
      "Item-by-item judgments with analysis:\n",
      "\n",
      "  [YES] Is the response written as a haiku (presented in three lines)?\n",
      "       → The generated text is presented in three separate lines: \"Crimson leaves drift d...\n",
      "\n",
      "  [YES] Does the haiku follow the traditional 5-7-5 syllable pattern?\n",
      "       → Line 1: \"Crimson leaves drift down\" = crimson (2) + leaves (1) + drift (1) + dow...\n",
      "\n",
      "  [YES] Is the subject of the haiku clearly about autumn leaves falling?\n",
      "       → The haiku explicitly names 'Crimson leaves' and describes them 'drift[ing] down,...\n",
      "\n",
      "  [YES] Does the response contain only the haiku (no additional unrelated explanation or text)?\n",
      "       → The generated text consists solely of a three-line haiku about autumn leaves fal...\n"
     ]
    }
   ],
   "source": [
    "# ChecklistScorer (item mode): One LLM call per question (original TICK methodology)\n",
    "item_scorer = ChecklistScorer(mode=\"item\", capture_reasoning=True, model=MODEL, temperature=0.0)\n",
    "\n",
    "# Use the same checklist from earlier\n",
    "score = item_scorer.score(checklist, target=good_response, input=input_text)\n",
    "\n",
    "print(f\"Item-mode scorer results:\")\n",
    "print(f\"  Pass rate: {score.pass_rate:.0%}\")\n",
    "print(f\"  LLM calls made: {score.metadata['num_calls']}\")\n",
    "print()\n",
    "print(\"Item-by-item judgments with analysis:\")\n",
    "for item, item_score in zip(checklist.items, score.item_scores):\n",
    "    analysis = item_score.reasoning[:80] + \"...\" if item_score.reasoning and len(item_score.reasoning) > 80 else item_score.reasoning\n",
    "    print(f\"\\n  [{item_score.answer.name:3}] {item.question}\")\n",
    "    if analysis:\n",
    "        print(f\"       → {analysis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. RLCF pipelines + ChecklistScorer (weighted mode)\n",
    "\n",
    "**RLCF** generates checklists with weighted items (0-100 importance) via `DirectGenerator` (direct mode) or `ContrastiveGenerator` (candidate modes).\n",
    "\n",
    "Two main modes:\n",
    "- **rlcf_direct** (DirectGenerator): Uses input + reference response\n",
    "- **rlcf_candidate** (ContrastiveGenerator): Also analyzes candidate responses to find failure modes\n",
    "  - Pass explicit candidates, OR\n",
    "  - Auto-generate candidates using smaller models (recommended, matches original paper)\n",
    "\n",
    "**ChecklistScorer with `primary_metric=\"weighted\"`** uses item weights for weighted average scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:22:14.080090Z",
     "iopub.status.busy": "2026-02-27T06:22:14.079807Z",
     "iopub.status.idle": "2026-02-27T06:22:42.941463Z",
     "shell.execute_reply": "2026-02-27T06:22:42.939971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Write a Python function that calculates the factorial of a number.\n",
      "\n",
      "Generated 7 weighted checklist items:\n",
      "  [100] Is the provided code syntactically valid Python code (would parse/run without syntax errors)?\n",
      "  [100] Does the response define a Python function (i.e., contains a def statement)?\n",
      "  [ 85] Does the defined function accept one argument (a parameter for the number whose factorial is to be calculated)?\n",
      "  [100] For input 0, does the function return 1?\n",
      "  [ 95] For input 5, does the function return 120?\n",
      "  [ 90] Does the function return the factorial value (use a return statement) rather than only printing it?\n",
      "  [ 80] Is the function self-contained (does not reference undefined helper functions or missing imports)?\n"
     ]
    }
   ],
   "source": [
    "# RLCF Direct Mode (via DirectGenerator)\n",
    "rlcf = DirectGenerator(method_name=\"rlcf_direct\", model=MODEL, max_tokens=4096)\n",
    "\n",
    "input_text = \"Write a Python function that calculates the factorial of a number.\"\n",
    "reference = \"\"\"def factorial(n):\n",
    "    if n < 0:\n",
    "        raise ValueError(\"Factorial not defined for negative numbers\")\n",
    "    if n <= 1:\n",
    "        return 1\n",
    "    return n * factorial(n - 1)\"\"\"\n",
    "\n",
    "checklist = rlcf.generate(input=input_text, reference=reference)\n",
    "\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"\\nGenerated {len(checklist.items)} weighted checklist items:\")\n",
    "for item in checklist.items:\n",
    "    print(f\"  [{item.weight:3.0f}] {item.question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:22:42.944738Z",
     "iopub.status.busy": "2026-02-27T06:22:42.944436Z",
     "iopub.status.idle": "2026-02-27T06:23:08.745184Z",
     "shell.execute_reply": "2026-02-27T06:23:08.743929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOD Implementation:\n",
      "def factorial(n):\n",
      "    if n < 0:\n",
      "        raise ValueError(\"n must be non-negative\")\n",
      "    result = 1\n",
      "    for i in range(2, n + 1):\n",
      "        result *= i\n",
      "    return result\n",
      "\n",
      "Weighted score: 100%\n",
      "Unweighted score: 100%\n"
     ]
    }
   ],
   "source": [
    "# Score with ChecklistScorer (weighted mode)\n",
    "weighted_scorer = ChecklistScorer(mode=\"item\", primary_metric=\"weighted\", model=MODEL)\n",
    "\n",
    "# Good implementation\n",
    "good_impl = \"\"\"def factorial(n):\n",
    "    if n < 0:\n",
    "        raise ValueError(\"n must be non-negative\")\n",
    "    result = 1\n",
    "    for i in range(2, n + 1):\n",
    "        result *= i\n",
    "    return result\"\"\"\n",
    "\n",
    "score = weighted_scorer.score(checklist, target=good_impl, input=input_text)\n",
    "\n",
    "print(\"GOOD Implementation:\")\n",
    "print(good_impl)\n",
    "print(f\"\\nWeighted score: {score.weighted_score:.0%}\")\n",
    "print(f\"Unweighted score: {score.total_score:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:23:08.748959Z",
     "iopub.status.busy": "2026-02-27T06:23:08.748679Z",
     "iopub.status.idle": "2026-02-27T06:23:45.453894Z",
     "shell.execute_reply": "2026-02-27T06:23:45.452563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUGGY Implementation:\n",
      "def factorial(n):\n",
      "    return n * factorial(n - 1)\n",
      "\n",
      "Weighted score: 56%\n",
      "Unweighted score: 57%\n",
      "\n",
      "Item scores:\n",
      "  [ChecklistItemAnswer.YES] (weight=100) Is the provided code syntactically valid Python code (would parse/run without syntax errors)?\n",
      "  [ChecklistItemAnswer.YES] (weight=100) Does the response define a Python function (i.e., contains a def statement)?\n",
      "  [ChecklistItemAnswer.YES] (weight= 85) Does the defined function accept one argument (a parameter for the number whose factorial is to be calculated)?\n",
      "  [ChecklistItemAnswer.NO] (weight=100) For input 0, does the function return 1?\n",
      "  [ChecklistItemAnswer.NO] (weight= 95) For input 5, does the function return 120?\n",
      "  [ChecklistItemAnswer.NO] (weight= 90) Does the function return the factorial value (use a return statement) rather than only printing it?\n",
      "  [ChecklistItemAnswer.YES] (weight= 80) Is the function self-contained (does not reference undefined helper functions or missing imports)?\n"
     ]
    }
   ],
   "source": [
    "# Buggy implementation (missing edge cases)\n",
    "buggy_impl = \"\"\"def factorial(n):\n",
    "    return n * factorial(n - 1)\"\"\"\n",
    "\n",
    "score = weighted_scorer.score(checklist, target=buggy_impl, input=input_text)\n",
    "\n",
    "print(\"BUGGY Implementation:\")\n",
    "print(buggy_impl)\n",
    "print(f\"\\nWeighted score: {score.weighted_score:.0%}\")\n",
    "print(f\"Unweighted score: {score.total_score:.0%}\")\n",
    "print(\"\\nItem scores:\")\n",
    "for item, item_score in zip(checklist.items, score.item_scores):\n",
    "    print(f\"  [{item_score.answer}] (weight={item.weight:3.0f}) {item.question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:23:45.456917Z",
     "iopub.status.busy": "2026-02-27T06:23:45.456612Z",
     "iopub.status.idle": "2026-02-27T06:24:08.525165Z",
     "shell.execute_reply": "2026-02-27T06:24:08.523704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate-based mode (analyzes failure patterns):\n",
      "Generated 5 items:\n",
      "  [100] Does the response contain a valid Python function definition (a def statement with a function name and parameter list)?\n",
      "  [100] Does the function return 1 when called with 0 (i.e., factorial(0) == 1)?\n",
      "  [100] Does the function return the correct factorial for a typical positive integer input (e.g., factorial(5) == 120)?\n",
      "  [ 90] Is the provided code syntactically valid Python as given (parsable and not referencing undefined names or missing required imports)?\n",
      "  [ 40] Does the function explicitly handle negative inputs (for example by raising an error or documenting a defined behavior)?\n"
     ]
    }
   ],
   "source": [
    "# RLCF Candidate-Based Mode (finds failure modes from examples)\n",
    "rlcf_candidate = ContrastiveGenerator(method_name=\"rlcf_candidate\", model=MODEL)\n",
    "\n",
    "# Candidate responses with various issues\n",
    "candidates = [\n",
    "    \"def factorial(n): return n * factorial(n-1)\",  # No base case\n",
    "    \"def fact(n): return 1 if n == 0 else n * fact(n-1)\",  # Wrong function name\n",
    "    \"def factorial(n): return math.factorial(n)\",  # Uses library\n",
    "]\n",
    "\n",
    "checklist = rlcf_candidate.generate(\n",
    "    input=input_text,\n",
    "    reference=reference,\n",
    "    candidates=candidates\n",
    ")\n",
    "\n",
    "print(\"Candidate-based mode (analyzes failure patterns):\")\n",
    "print(f\"Generated {len(checklist.items)} items:\")\n",
    "for item in checklist.items:\n",
    "    print(f\"  [{item.weight:3.0f}] {item.question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:24:08.528121Z",
     "iopub.status.busy": "2026-02-27T06:24:08.527915Z",
     "iopub.status.idle": "2026-02-27T06:24:39.723407Z",
     "shell.execute_reply": "2026-02-27T06:24:39.722460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-candidate mode (generates candidates automatically):\n",
      "Using 2 auto-generated candidates\n",
      "\n",
      "============================================================\n",
      "AUTO-GENERATED CANDIDATES:\n",
      "============================================================\n",
      "\n",
      "--- Candidate 1 ---\n",
      "Certainly! Below is a Python function that calculates the factorial of a given non-negative integer using a recursive approach:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Factorial is not defined for negative numbers.\")\n",
      "    elif n == 0 or n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "...\n",
      "\n",
      "--- Candidate 2 ---\n",
      "Certainly! Here is a Python function that calculates the factorial of a given non-negative integer using a recursive approach:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    \"\"\"Calculate the factorial of a non-negative integer n.\"\"\"\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Factorial is not defined for negative num...\n",
      "\n",
      "============================================================\n",
      "GENERATED CHECKLIST (5 items):\n",
      "============================================================\n",
      "  [100] Does the response include syntactically valid Python code that defines a function?\n",
      "  [ 90] Does the defined function accept exactly one parameter?\n",
      "  [ 95] Does the function return 1 for input 0 and for input 1?\n",
      "  [100] Does the function compute the factorial correctly for a positive integer input (for example, return 120 for input 5)?\n",
      "  [ 50] Does the function handle negative inputs by raising an exception or otherwise signaling an error?\n"
     ]
    }
   ],
   "source": [
    "# RLCF with Auto-Generated Candidates (Recommended)\n",
    "# Instead of providing explicit candidates, let ContrastiveGenerator auto-generate them\n",
    "# This matches the original RLCF paper methodology\n",
    "\n",
    "rlcf_auto = ContrastiveGenerator(\n",
    "    method_name=\"rlcf_candidate\",\n",
    "    model=MODEL,\n",
    "    candidate_models=[\"openai/gpt-4o-mini\"],  # Smaller model(s) to generate candidates\n",
    "    num_candidates=2,  # Number of candidates to generate (when using single model)\n",
    ")\n",
    "\n",
    "auto_checklist = rlcf_auto.generate(\n",
    "    input=input_text,\n",
    "    reference=reference,\n",
    "    # No candidates argument - they're auto-generated!\n",
    ")\n",
    "\n",
    "print(\"Auto-candidate mode (generates candidates automatically):\")\n",
    "print(f\"Using {auto_checklist.metadata.get('num_candidates')} auto-generated candidates\\n\")\n",
    "\n",
    "# Print the auto-generated candidates\n",
    "print(\"=\" * 60)\n",
    "print(\"AUTO-GENERATED CANDIDATES:\")\n",
    "print(\"=\" * 60)\n",
    "for i, candidate in enumerate(auto_checklist.metadata.get(\"candidates\", []), 1):\n",
    "    print(f\"\\n--- Candidate {i} ---\")\n",
    "    print(candidate[:300] + \"...\" if len(candidate) > 300 else candidate)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"GENERATED CHECKLIST ({len(auto_checklist.items)} items):\")\n",
    "print(\"=\" * 60)\n",
    "for item in auto_checklist.items:\n",
    "    print(f\"  [{item.weight:3.0f}] {item.question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:24:39.725734Z",
     "iopub.status.busy": "2026-02-27T06:24:39.725457Z",
     "iopub.status.idle": "2026-02-27T06:25:21.339865Z",
     "shell.execute_reply": "2026-02-27T06:25:21.338512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring with AUTO-CANDIDATE checklist:\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GOOD Implementation:\n",
      "def factorial(n):\n",
      "    if n < 0:\n",
      "        raise ValueError(\"n must be non-negative\")\n",
      "    result = 1\n",
      "    for i in range(2, n + 1):\n",
      "        result *= i\n",
      "    return result\n",
      "\n",
      "Weighted score: 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "BUGGY Implementation:\n",
      "def factorial(n):\n",
      "    return n * factorial(n - 1)\n",
      "\n",
      "Weighted score: 44%\n",
      "\n",
      "Item breakdown:\n",
      "  [ChecklistItemAnswer.YES] (weight=100) Does the response include syntactically valid Python code that defines a function?\n",
      "  [ChecklistItemAnswer.YES] (weight= 90) Does the defined function accept exactly one parameter?\n",
      "  [ChecklistItemAnswer.NO] (weight= 95) Does the function return 1 for input 0 and for input 1?\n",
      "  [ChecklistItemAnswer.NO] (weight=100) Does the function compute the factorial correctly for a positive integer input (for example, return 120 for input 5)?\n",
      "  [ChecklistItemAnswer.NO] (weight= 50) Does the function handle negative inputs by raising an exception or otherwise signaling an error?\n"
     ]
    }
   ],
   "source": [
    "# Score using the auto-generated candidate checklist\n",
    "print(\"Scoring with AUTO-CANDIDATE checklist:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Score the good implementation\n",
    "score = weighted_scorer.score(auto_checklist, target=good_impl, input=input_text)\n",
    "print(\"\\nGOOD Implementation:\")\n",
    "print(good_impl)\n",
    "print(f\"\\nWeighted score: {score.weighted_score:.0%}\")\n",
    "\n",
    "# Score the buggy implementation  \n",
    "score = weighted_scorer.score(auto_checklist, target=buggy_impl, input=input_text)\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"\\nBUGGY Implementation:\")\n",
    "print(buggy_impl)\n",
    "print(f\"\\nWeighted score: {score.weighted_score:.0%}\")\n",
    "print(\"\\nItem breakdown:\")\n",
    "for item, item_score in zip(auto_checklist.items, score.item_scores):\n",
    "    print(f\"  [{item_score.answer}] (weight={item.weight:3.0f}) {item.question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RLCF Candidates-Only Mode (No Reference)\n",
    "\n",
    "When you don't have a reference/gold-standard response, `ContrastiveGenerator` can still generate checklists\n",
    "by analyzing auto-generated candidates. This is the `rlcf_candidates_only` preset.\n",
    "\n",
    "**Use case:** Evaluating responses to open-ended questions where no \"correct\" answer exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:25:21.342977Z",
     "iopub.status.busy": "2026-02-27T06:25:21.342682Z",
     "iopub.status.idle": "2026-02-27T06:25:49.980880Z",
     "shell.execute_reply": "2026-02-27T06:25:49.979626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CANDIDATES-ONLY MODE (no reference)\n",
      "============================================================\n",
      "Input: Explain why the sky is blue in a way a 5-year-old would understand.\n",
      "\n",
      "Metadata shows candidates_only=None\n",
      "Number of auto-generated candidates: 3\n",
      "\n",
      "Generated 7 weighted checklist items:\n",
      "  [100] Does the response give a clear explanation of why the sky appears blue (i.e., links sunlight + atmosphere to the blue appearance)?\n",
      "  [100] Is the explanation factually accurate with no major scientific errors about the cause of the sky's color?\n",
      "  [ 90] Does the response state that sunlight contains multiple colors (or that white light is made of many colors)?\n",
      "  [ 95] Does the response state that blue light is scattered more by the air/atmosphere than other colors (or convey this idea in age-appropriate terms)?\n",
      "  [ 90] Does the response use vocabulary and short sentence structure appropriate for a typical 5-year-old (simple words, no complex sentences)?\n",
      "  [ 80] Does the response avoid unexplained technical jargon (or explain any necessary term in a kid-friendly way)?\n",
      "  [ 60] Does the response include an age-appropriate analogy or example to help a 5-year-old understand (e.g., crayons, bouncing balls, paint)?\n"
     ]
    }
   ],
   "source": [
    "# RLCF Candidates-Only Mode (No Reference Required)\n",
    "# Useful when you don't have a gold-standard reference response\n",
    "\n",
    "rlcf_no_ref = ContrastiveGenerator(\n",
    "    method_name=\"rlcf_candidates_only\",\n",
    "    model=MODEL,\n",
    "    candidate_models=[\"openai/gpt-4o-mini\"],\n",
    "    num_candidates=3,\n",
    ")\n",
    "\n",
    "# Open-ended input with no single \"correct\" answer\n",
    "open_input = \"Explain why the sky is blue in a way a 5-year-old would understand.\"\n",
    "\n",
    "# Generate checklist WITHOUT providing a reference\n",
    "# Candidates are auto-generated and compared to each other to identify criteria\n",
    "no_ref_checklist = rlcf_no_ref.generate(input=open_input)\n",
    "# Note: No reference argument! \n",
    "\n",
    "print(\"CANDIDATES-ONLY MODE (no reference)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input: {open_input}\")\n",
    "print(f\"\\nMetadata shows candidates_only={no_ref_checklist.metadata.get('candidates_only')}\")\n",
    "print(f\"Number of auto-generated candidates: {no_ref_checklist.metadata.get('num_candidates')}\")\n",
    "print(f\"\\nGenerated {len(no_ref_checklist.items)} weighted checklist items:\")\n",
    "for item in no_ref_checklist.items:\n",
    "    print(f\"  [{item.weight:3.0f}] {item.question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:25:49.983693Z",
     "iopub.status.busy": "2026-02-27T06:25:49.983400Z",
     "iopub.status.idle": "2026-02-27T06:27:03.448404Z",
     "shell.execute_reply": "2026-02-27T06:27:03.447764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring with CANDIDATES-ONLY checklist:\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response 1: The sky is blue because tiny bits of air scatter blue light from the s...\n",
      "Weighted score: 85%\n",
      "Item breakdown:\n",
      "  [ChecklistItemAnswer.YES] (weight=100) Does the response give a clear explanation of why the s...\n",
      "  [ChecklistItemAnswer.YES] (weight=100) Is the explanation factually accurate with no major sci...\n",
      "  [ChecklistItemAnswer.YES] (weight= 90) Does the response state that sunlight contains multiple...\n",
      "  [ChecklistItemAnswer.YES] (weight= 95) Does the response state that blue light is scattered mo...\n",
      "  [ChecklistItemAnswer.NO] (weight= 90) Does the response use vocabulary and short sentence str...\n",
      "  [ChecklistItemAnswer.YES] (weight= 80) Does the response avoid unexplained technical jargon (o...\n",
      "  [ChecklistItemAnswer.YES] (weight= 60) Does the response include an age-appropriate analogy or...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response 2: Due to Rayleigh scattering, shorter wavelength electromagnetic radiati...\n",
      "Weighted score: 32%\n",
      "Item breakdown:\n",
      "  [ChecklistItemAnswer.NO] (weight=100) Does the response give a clear explanation of why the s...\n",
      "  [ChecklistItemAnswer.YES] (weight=100) Is the explanation factually accurate with no major sci...\n",
      "  [ChecklistItemAnswer.NO] (weight= 90) Does the response state that sunlight contains multiple...\n",
      "  [ChecklistItemAnswer.YES] (weight= 95) Does the response state that blue light is scattered mo...\n",
      "  [ChecklistItemAnswer.NO] (weight= 90) Does the response use vocabulary and short sentence str...\n",
      "  [ChecklistItemAnswer.NO] (weight= 80) Does the response avoid unexplained technical jargon (o...\n",
      "  [ChecklistItemAnswer.NO] (weight= 60) Does the response include an age-appropriate analogy or...\n"
     ]
    }
   ],
   "source": [
    "# Score responses using the candidates-only checklist\n",
    "\n",
    "print(\"Scoring with CANDIDATES-ONLY checklist:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# A child-friendly explanation\n",
    "response1 = \"\"\"The sky is blue because tiny bits of air scatter blue light from the sun \n",
    "more than other colors, like how glitter spreads everywhere when you throw it!\"\"\"\n",
    "\n",
    "# A too-technical explanation (not appropriate for a 5-year-old)\n",
    "response2 = \"\"\"Due to Rayleigh scattering, shorter wavelength electromagnetic radiation \n",
    "is scattered more efficiently by atmospheric molecules, resulting in the preferential \n",
    "scattering of blue light (λ ≈ 450-495 nm) compared to longer wavelengths.\"\"\"\n",
    "\n",
    "for i, resp in enumerate([response1, response2], 1):\n",
    "    score = weighted_scorer.score(no_ref_checklist, target=resp, input=open_input)\n",
    "    print(f\"\\nResponse {i}: {resp[:70].strip()}...\")\n",
    "    print(f\"Weighted score: {score.weighted_score:.0%}\")\n",
    "    print(\"Item breakdown:\")\n",
    "    for item, item_score in zip(no_ref_checklist.items, score.item_scores):\n",
    "        print(f\"  [{item_score.answer}] (weight={item.weight:3.0f}) {item.question[:55]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. RocketEval pipeline + ChecklistScorer (logprobs mode)\n",
    "\n",
    "**RocketEval pipeline** generates checklists from query + reference response via `DirectGenerator`.\n",
    "\n",
    "**ChecklistScorer with `use_logprobs=True`** uses logprobs to calculate confidence levels:\n",
    "- YES_90: Very confident yes (≥85%)\n",
    "- YES_70: Confident yes (65-85%)\n",
    "- UNSURE: Uncertain (35-65%)\n",
    "- NO_30: Confident no (15-35%)\n",
    "- NO_10: Very confident no (<15%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:27:03.450163Z",
     "iopub.status.busy": "2026-02-27T06:27:03.450018Z",
     "iopub.status.idle": "2026-02-27T06:27:19.042472Z",
     "shell.execute_reply": "2026-02-27T06:27:19.041228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: What are the three laws of thermodynamics? Explain each briefly.\n",
      "\n",
      "Generated 8 checklist items:\n",
      "  1. Does the response list exactly three distinct laws (no more, no fewer)?\n",
      "  2. Is the First Law described as conservation of energy—energy cannot be created or destroyed and the total energy of an isolated system is constant?\n",
      "  3. Is the Second Law described as the tendency for total entropy of an isolated system/universe to increase (or never decrease) and that heat flows spontaneously from hot to cold?\n",
      "  4. Is the Third Law described as the entropy of a perfect crystal approaching zero as temperature approaches absolute zero and that absolute zero cannot be reached in a finite number of steps?\n",
      "  5. Are each of the three laws clearly labeled (First/Second/Third) and given a brief explanation?\n",
      "  6. Are the explanations concise (short, to the point) rather than long derivations or unrelated detail?\n",
      "  7. Are there no major factual errors or misleading absolute statements (e.g., claiming entropy always increases for non-isolated or reversible processes without qualification)?\n",
      "  8. Does the response use correct technical terms appropriately (e.g., entropy, isolated system/universe, absolute zero, perfect crystal) and avoid irrelevant extra laws or unrelated content?\n"
     ]
    }
   ],
   "source": [
    "# Initialize RocketEval generator\n",
    "rocketeval = DirectGenerator(method_name=\"rocketeval\", model=MODEL)\n",
    "\n",
    "input_text = \"What are the three laws of thermodynamics? Explain each briefly.\"\n",
    "reference = \"\"\"The three laws of thermodynamics are:\n",
    "\n",
    "1. First Law (Conservation of Energy): Energy cannot be created or destroyed, only transformed. \n",
    "   The total energy of an isolated system remains constant.\n",
    "\n",
    "2. Second Law (Entropy): In any energy transfer, the total entropy of a system and its \n",
    "   surroundings always increases. Heat flows spontaneously from hot to cold.\n",
    "\n",
    "3. Third Law (Absolute Zero): As temperature approaches absolute zero, the entropy of a \n",
    "   perfect crystal approaches zero. It's impossible to reach absolute zero in finite steps.\"\"\"\n",
    "\n",
    "checklist = rocketeval.generate(input=input_text, reference=reference)\n",
    "\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"\\nGenerated {len(checklist.items)} checklist items:\")\n",
    "for i, item in enumerate(checklist.items, 1):\n",
    "    print(f\"  {i}. {item.question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:27:19.045697Z",
     "iopub.status.busy": "2026-02-27T06:27:19.045355Z",
     "iopub.status.idle": "2026-02-27T06:27:25.375457Z",
     "shell.execute_reply": "2026-02-27T06:27:25.374051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOD Response:\n",
      "The three laws of thermodynamics:\n",
      "\n",
      "1. First Law: Energy is conserved - it can change forms but the total amount stays constant.\n",
      "\n",
      "2. Second Law: Entropy (disorder) always increases in isolated systems....\n",
      "\n",
      "Normalized score: 50%\n",
      "\n",
      "Confidence levels:\n",
      "  [YES_90] Does the response list exactly three distinct laws (no more,...\n",
      "  [NO_10 ] Is the First Law described as conservation of energy—energy ...\n",
      "  [NO_10 ] Is the Second Law described as the tendency for total entrop...\n",
      "  [NO_10 ] Is the Third Law described as the entropy of a perfect cryst...\n",
      "  [YES_90] Are each of the three laws clearly labeled (First/Second/Thi...\n",
      "  [YES_90] Are the explanations concise (short, to the point) rather th...\n",
      "  [NO_10 ] Are there no major factual errors or misleading absolute sta...\n",
      "  [YES_90] Does the response use correct technical terms appropriately ...\n"
     ]
    }
   ],
   "source": [
    "# Score with ChecklistScorer (logprobs mode)\n",
    "normalized_scorer = ChecklistScorer(mode=\"item\", use_logprobs=True, primary_metric=\"normalized\", model=\"openai/gpt-4o-mini\")\n",
    "\n",
    "# Good response\n",
    "good_response = \"\"\"The three laws of thermodynamics:\n",
    "\n",
    "1. First Law: Energy is conserved - it can change forms but the total amount stays constant.\n",
    "\n",
    "2. Second Law: Entropy (disorder) always increases in isolated systems. This is why heat flows from hot to cold and not the reverse.\n",
    "\n",
    "3. Third Law: At absolute zero temperature, a perfect crystal has zero entropy. We can never actually reach absolute zero.\"\"\"\n",
    "\n",
    "score = normalized_scorer.score(checklist, target=good_response, input=input_text)\n",
    "\n",
    "print(\"GOOD Response:\")\n",
    "print(good_response[:200] + \"...\")\n",
    "print(f\"\\nNormalized score: {score.normalized_score:.0%}\")\n",
    "print(\"\\nConfidence levels:\")\n",
    "for item, item_score in zip(checklist.items, score.item_scores):\n",
    "    print(f\"  [{item_score.confidence_level.name:6}] {item.question[:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:27:25.378389Z",
     "iopub.status.busy": "2026-02-27T06:27:25.378155Z",
     "iopub.status.idle": "2026-02-27T06:27:31.184600Z",
     "shell.execute_reply": "2026-02-27T06:27:31.183416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTIAL Response (missing third law):\n",
      "The laws of thermodynamics:\n",
      "\n",
      "The first law says energy is conserved. You can't create or destroy energy.\n",
      "\n",
      "The second law is about entropy always increasing.\n",
      "\n",
      "Normalized score: 12%\n",
      "\n",
      "Confidence levels:\n",
      "  [NO_10 ] Does the response list exactly three distinct laws (no more,...\n",
      "  [NO_10 ] Is the First Law described as conservation of energy—energy ...\n",
      "  [NO_10 ] Is the Second Law described as the tendency for total entrop...\n",
      "  [NO_10 ] Is the Third Law described as the entropy of a perfect cryst...\n",
      "  [NO_10 ] Are each of the three laws clearly labeled (First/Second/Thi...\n",
      "  [YES_90] Are the explanations concise (short, to the point) rather th...\n",
      "  [NO_10 ] Are there no major factual errors or misleading absolute sta...\n",
      "  [NO_10 ] Does the response use correct technical terms appropriately ...\n"
     ]
    }
   ],
   "source": [
    "# Partial response (missing some laws)\n",
    "partial_response = \"\"\"The laws of thermodynamics:\n",
    "\n",
    "The first law says energy is conserved. You can't create or destroy energy.\n",
    "\n",
    "The second law is about entropy always increasing.\"\"\"\n",
    "\n",
    "score = normalized_scorer.score(checklist, target=partial_response, input=input_text)\n",
    "\n",
    "print(\"PARTIAL Response (missing third law):\")\n",
    "print(partial_response)\n",
    "print(f\"\\nNormalized score: {score.normalized_score:.0%}\")\n",
    "print(\"\\nConfidence levels:\")\n",
    "for item, item_score in zip(checklist.items, score.item_scores):\n",
    "    level = item_score.confidence_level.name if item_score.confidence_level else \"N/A\"\n",
    "    print(f\"  [{level:6}] {item.question[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChecklistScorer Without Logprobs Support\n",
    "\n",
    "Not all models support logprobs output. When you use `use_logprobs=True` with a model that doesn't support logprobs:\n",
    "\n",
    "1. A **warning is emitted at initialization** (not per-item)\n",
    "2. Scoring falls back to **text-based Yes/No parsing**\n",
    "3. `confidence` and `confidence_level` are set to **`None`**\n",
    "4. `normalized_score` **equals `pass_rate`** (unweighted scoring)\n",
    "\n",
    "This matches the RocketEval paper's behavior of returning `NaN` when logprobs are unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:27:31.187567Z",
     "iopub.status.busy": "2026-02-27T06:27:31.187284Z",
     "iopub.status.idle": "2026-02-27T06:28:24.487710Z",
     "shell.execute_reply": "2026-02-27T06:28:24.486420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Model 'openai/gpt-5-mini' does not support logprobs. Scorer will use text-based scoring with confidence=None. normalized_score will equal pass_rate (unweighted).\n",
      "\n",
      "_logprobs_available = False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring without logprobs:\n",
      "  Pass rate: 75%\n",
      "  Normalized score: 75%  (equals pass_rate when no logprobs)\n",
      "\n",
      "Item scores (confidence and confidence_level are None):\n",
      "  [yes] confidence=None, level=None\n",
      "       Does the response list exactly three distinct laws (no ...\n",
      "  [yes] confidence=None, level=None\n",
      "       Is the First Law described as conservation of energy—en...\n",
      "  [yes] confidence=None, level=None\n",
      "       Is the Second Law described as the tendency for total e...\n",
      "  [no ] confidence=None, level=None\n",
      "       Is the Third Law described as the entropy of a perfect ...\n",
      "  [yes] confidence=None, level=None\n",
      "       Are each of the three laws clearly labeled (First/Secon...\n",
      "  [yes] confidence=None, level=None\n",
      "       Are the explanations concise (short, to the point) rath...\n",
      "  [no ] confidence=None, level=None\n",
      "       Are there no major factual errors or misleading absolut...\n",
      "  [yes] confidence=None, level=None\n",
      "       Does the response use correct technical terms appropria...\n"
     ]
    }
   ],
   "source": [
    "# ChecklistScorer with use_logprobs=True but a model that does NOT support logprobs\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.simplefilter(\"always\")\n",
    "    scorer_no_logprobs = ChecklistScorer(mode=\"item\", use_logprobs=True, primary_metric=\"normalized\", model=\"openai/gpt-5-mini\")\n",
    "    \n",
    "    if w:\n",
    "        print(f\"WARNING: {w[0].message}\")\n",
    "    print(f\"\\n_logprobs_available = {scorer_no_logprobs._logprobs_available}\")\n",
    "\n",
    "# Score using the same checklist\n",
    "score = scorer_no_logprobs.score(checklist, target=good_response, input=input_text)\n",
    "\n",
    "print(f\"\\nScoring without logprobs:\")\n",
    "print(f\"  Pass rate: {score.pass_rate:.0%}\")\n",
    "print(f\"  Normalized score: {score.normalized_score:.0%}  (equals pass_rate when no logprobs)\")\n",
    "\n",
    "print(f\"\\nItem scores (confidence and confidence_level are None):\")\n",
    "for item, item_score in zip(checklist.items, score.item_scores):\n",
    "    print(f\"  [{item_score.answer.value:3}] confidence={item_score.confidence}, level={item_score.confidence_level}\")\n",
    "    print(f\"       {item.question[:55]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus: InteractEval 1-5 Scaling\n",
    "\n",
    "The `Score` model includes a `scaled_score_1_5` property that scales the pass_rate (0-1) to a 1-5 score.\n",
    "This follows the InteractEval paper's scaling formula: `score = pass_rate * 4 + 1`\n",
    "\n",
    "This is useful when you want to report scores on a more intuitive 1-5 scale (like star ratings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:28:24.490379Z",
     "iopub.status.busy": "2026-02-27T06:28:24.490152Z",
     "iopub.status.idle": "2026-02-27T06:28:51.866661Z",
     "shell.execute_reply": "2026-02-27T06:28:51.865118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InteractEval 1-5 Scaling Demo\n",
      "============================================================\n",
      "Formula: scaled_score = pass_rate * 4 + 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfect haiku:\n",
      "  Pass rate: 100%\n",
      "  Scaled (1-5): 5.00\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prose instead:\n",
      "  Pass rate: 40%\n",
      "  Scaled (1-5): 2.60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate 1-5 scaling using scores from earlier examples\n",
    "tick = DirectGenerator(method_name=\"tick\", model=MODEL)\n",
    "scorer = ChecklistScorer(mode=\"batch\", model=MODEL)\n",
    "\n",
    "input_text = \"Write a haiku about autumn leaves.\"\n",
    "checklist = tick.generate(input=input_text)\n",
    "\n",
    "# Score different quality responses\n",
    "examples = [\n",
    "    (\"Perfect haiku\", \"Crimson leaves descend\\nDancing on autumn's last breath\\nEarth embraces gold\"),\n",
    "    (\"Prose instead\", \"Autumn leaves are beautiful. I enjoy watching them fall.\"),\n",
    "]\n",
    "\n",
    "print(\"InteractEval 1-5 Scaling Demo\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Formula: scaled_score = pass_rate * 4 + 1\")\n",
    "print()\n",
    "\n",
    "for name, response in examples:\n",
    "    score = scorer.score(checklist, target=response, input=input_text)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Pass rate: {score.pass_rate:.0%}\")\n",
    "    print(f\"  Scaled (1-5): {score.scaled_score_1_5:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Profile | Generator | Scorer Config | Use Case |\n",
    "|--------|-----------|--------------|----------|\n",
    "| **tick** | DirectGenerator | `ChecklistScorer(mode=\"batch\")` | Quick evaluation, no reference needed |\n",
    "| **tick** | DirectGenerator | `ChecklistScorer(mode=\"item\", capture_reasoning=True)` | Per-item reasoning (TICK paper) |\n",
    "| **rlcf_direct** | DirectGenerator | `ChecklistScorer(mode=\"item\", primary_metric=\"weighted\")` | Detailed evaluation with importance weights |\n",
    "| **rlcf_candidate** | ContrastiveGenerator | `ChecklistScorer(mode=\"item\", primary_metric=\"weighted\")` | Identifies failure modes from examples |\n",
    "| **rlcf_candidates_only** | ContrastiveGenerator | `ChecklistScorer(mode=\"item\", primary_metric=\"weighted\")` | Open-ended tasks without gold standard |\n",
    "| **rocketeval** | DirectGenerator | `ChecklistScorer(mode=\"item\", use_logprobs=True, primary_metric=\"normalized\")` | Confidence-aware evaluation |\n",
    "\n",
    "### Scorer Modes\n",
    "- **`mode=\"batch\"`**: Evaluates ALL checklist items in one LLM call (efficient)\n",
    "- **`mode=\"item\"`**: Evaluates ONE item per LLM call (more accurate, supports reasoning/logprobs)\n",
    "\n",
    "### RLCF Modes (via ContrastiveGenerator)\n",
    "- **rlcf_candidate**: Analyzes candidate responses to identify failure modes\n",
    "  - `candidates=[...]`: Pass explicit candidates\n",
    "  - `candidate_models=[...]`: Auto-generate candidates using smaller models (recommended)\n",
    "  - With reference: Compares candidates to reference\n",
    "  - **Without reference** (`rlcf_candidates_only`): Compares candidates to each other"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
