{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Custom Components\n",
    "\n",
    "This tutorial shows two approaches to extend **autochecklist**:\n",
    "\n",
    "### Easy: Custom Prompts from `.md` Files\n",
    "Write a prompt template in a `.md` file and load it — no Python subclassing needed.\n",
    "- **Custom generator prompts** — Write your generation prompt with `{input}` placeholder\n",
    "- **Custom scorer prompts** — Write your scoring prompt with `{input}`, `{target}`, `{checklist}` placeholders\n",
    "\n",
    "### Advanced: Subclass from Base Classes\n",
    "For full control, create your own Python classes:\n",
    "- **Generators** — Create checklists from inputs\n",
    "- **Scorers** — Evaluate targets against checklists\n",
    "- **Refiners** — Improve checklists (filter, deduplicate, etc.)\n",
    "\n",
    "The library uses a **Template Method** pattern: base classes provide common infrastructure (LLM calls, metadata handling), and you only implement the domain-specific logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:02:50.177607Z",
     "iopub.status.busy": "2026-02-27T07:02:50.177419Z",
     "iopub.status.idle": "2026-02-27T07:02:50.186706Z",
     "shell.execute_reply": "2026-02-27T07:02:50.185831Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# OpenRouter is the default provider. Custom components automatically inherit\n",
    "# multi-provider support (provider, base_url, client, api_format params)\n",
    "# via the base class. See pipeline_demo.ipynb for provider examples.\n",
    "assert os.getenv(\"OPENROUTER_API_KEY\"), \"Please set OPENROUTER_API_KEY in .env file\"\n",
    "\n",
    "MODEL = \"openai/gpt-5-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Custom Prompts from `.md` Files (Easy Way)\n",
    "\n",
    "The simplest way to create a custom generator or scorer is to write a prompt template as a `.md` file. No Python subclassing needed.\n",
    "\n",
    "### Custom Generator Prompt\n",
    "\n",
    "Write a `.md` file with an `{input}` placeholder. The LLM response is parsed automatically (numbered lists, bulleted lists, `[[bracketed]]` format all work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:02:50.189331Z",
     "iopub.status.busy": "2026-02-27T07:02:50.189157Z",
     "iopub.status.idle": "2026-02-27T07:02:50.193243Z",
     "shell.execute_reply": "2026-02-27T07:02:50.192099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an evaluation expert. Generate a checklist of yes/no questions to evaluate an AI response.\n",
      "\n",
      "**Instruction**: {input}\n",
      "\n",
      "Create 3-5 concise yes/no questions that assess:\n",
      "- Whether the response addresses the instruction\n",
      "- Quality and correctness of the response\n",
      "- Completeness of the response\n",
      "\n",
      "Output each question on its own line as a numbered list:\n",
      "1. Does the response ...?\n",
      "2. Is the response ...?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's look at our custom generator prompt\n",
    "with open(\"my_generator.md\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:02:50.224395Z",
     "iopub.status.busy": "2026-02-27T07:02:50.224223Z",
     "iopub.status.idle": "2026-02-27T07:03:12.545922Z",
     "shell.execute_reply": "2026-02-27T07:03:12.544434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4 items:\n",
      "1. Does the response write a limerick specifically about a cat?\n",
      "2. Does the poem follow limerick form (five lines with an AABBA rhyme scheme)?\n",
      "3. Does the limerick maintain an appropriate meter/rhythm for a limerick?\n",
      "4. Is the language clear, grammatical, and engaging?\n"
     ]
    }
   ],
   "source": [
    "# Use it directly with DirectGenerator\n",
    "from pathlib import Path\n",
    "from autochecklist import DirectGenerator\n",
    "\n",
    "gen = DirectGenerator(custom_prompt=Path(\"my_generator.md\"), model=MODEL)\n",
    "checklist = gen.generate(input=\"Write a limerick about a cat\")\n",
    "\n",
    "print(f\"Generated {len(checklist)} items:\")\n",
    "print(checklist.to_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:03:12.549140Z",
     "iopub.status.busy": "2026-02-27T07:03:12.548826Z",
     "iopub.status.idle": "2026-02-27T07:03:41.531824Z",
     "shell.execute_reply": "2026-02-27T07:03:41.530227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass rate: 100%\n",
      "\n",
      "Checklist:\n",
      "1. Does the response follow the instruction to write a limerick about a cat?\n",
      "2. Is the response in limerick form (five lines with an AABBA rhyme scheme)?\n",
      "3. Is the language clear and free of grammatical errors?\n",
      "4. Is the limerick complete and coherent (five lines expressing a single idea)?\n"
     ]
    }
   ],
   "source": [
    "# Or use it via pipeline() — even easier\n",
    "# First register the custom generator, then use it by name\n",
    "from autochecklist import pipeline, register_custom_generator\n",
    "\n",
    "register_custom_generator(\"my_gen\", \"my_generator.md\")\n",
    "pipe = pipeline(\"my_gen\", generator_model=MODEL, scorer_model=MODEL)\n",
    "\n",
    "result = pipe(\n",
    "    input=\"Write a limerick about a cat\",\n",
    "    target=\"There once was a cat named Sue\\nWho loved to eat fish and stew\\nShe'd purr all day\\nIn a cozy way\\nAnd nap under skies of blue\",\n",
    ")\n",
    "\n",
    "print(f\"Pass rate: {result.pass_rate:.0%}\")\n",
    "print(f\"\\nChecklist:\")\n",
    "print(result.checklist.to_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Custom Scorer Prompt\n\n`ChecklistScorer` already accepts a `custom_prompt` parameter. You can register a custom scorer prompt via `register_custom_scorer()` and then use it by name in the pipeline.\n\nYour scorer prompt should use `{input}`, `{target}`, and `{checklist}` placeholders, and instruct the LLM to output `Q1: YES/NO, Q2: YES/NO, ...`\n\n`register_custom_scorer()` also accepts optional config kwargs:\n- `mode` — `\"batch\"` (default) or `\"item\"`\n- `primary_metric` — `\"pass\"` (default), `\"weighted\"`, or `\"normalized\"` (auto-enables logprobs)\n- `capture_reasoning` — include per-item reasoning"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:03:41.535278Z",
     "iopub.status.busy": "2026-02-27T07:03:41.534929Z",
     "iopub.status.idle": "2026-02-27T07:03:41.540459Z",
     "shell.execute_reply": "2026-02-27T07:03:41.538964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a strict but fair evaluator. Assess whether the response meets each criterion.\n",
      "\n",
      "**Instruction**: {input}\n",
      "\n",
      "**Response to evaluate**: {target}\n",
      "\n",
      "**Evaluation criteria**:\n",
      "{checklist}\n",
      "\n",
      "For each question above, determine if the response satisfies the criterion.\n",
      "Be strict — only answer YES if the criterion is clearly and fully met.\n",
      "\n",
      "Output your answers in this exact format:\n",
      "Q1: YES or NO\n",
      "Q2: YES or NO\n",
      "(continue for all questions)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at our custom scorer prompt\n",
    "with open(\"my_scorer.md\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:03:41.543249Z",
     "iopub.status.busy": "2026-02-27T07:03:41.542958Z",
     "iopub.status.idle": "2026-02-27T07:04:04.745951Z",
     "shell.execute_reply": "2026-02-27T07:04:04.744254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass rate: 100%\n",
      "\n",
      "Checklist:\n",
      "1. 1. Does the response provide a single-sentence explanation of photosynthesis?\n",
      "2. 2. Is the explanation factually accurate about converting light energy, carbon dioxide, and water into glucose and oxygen?\n",
      "3. 3. Is the sentence clear and easily understandable?\n",
      "4. 4. Does the sentence include the essential components or outcomes of photosynthesis (light energy, CO2, water, and the primary products) within the one-sentence constraint?\n",
      "\n",
      "Item-level results:\n",
      "  YES - 1. Does the response provide a single-sentence explanation of photosynthesis?\n",
      "  YES - 2. Is the explanation factually accurate about converting light energy, carbon dioxide, and water into glucose and oxygen?\n",
      "  YES - 3. Is the sentence clear and easily understandable?\n",
      "  YES - 4. Does the sentence include the essential components or outcomes of photosynthesis (light energy, CO2, water, and the primary products) within the one-sentence constraint?\n"
     ]
    }
   ],
   "source": [
    "# Use custom generator + custom scorer together via pipeline\n",
    "# Register both custom prompts, then use them by name\n",
    "from autochecklist import register_custom_scorer\n",
    "\n",
    "register_custom_generator(\"my_eval\", \"my_generator.md\")\n",
    "register_custom_scorer(\"strict_eval\", \"my_scorer.md\")\n",
    "\n",
    "pipe = pipeline(\"my_eval\", generator_model=MODEL, scorer=\"strict_eval\")\n",
    "\n",
    "result = pipe(\n",
    "    input=\"Explain photosynthesis in one sentence\",\n",
    "    target=\"Photosynthesis is when plants use sunlight, water, and CO2 to make glucose and oxygen.\",\n",
    ")\n",
    "\n",
    "print(f\"Pass rate: {result.pass_rate:.0%}\")\n",
    "print(f\"\\nChecklist:\")\n",
    "print(result.checklist.to_text())\n",
    "print(f\"\\nItem-level results:\")\n",
    "for item, score in zip(result.checklist.items, result.score.item_scores):\n",
    "    print(f\"  {score.answer.value.upper():3} - {item.question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering Custom Prompts for Reuse\n",
    "\n",
    "Register a custom generator or scorer under a name so you can use it like any built-in method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:04:04.749190Z",
     "iopub.status.busy": "2026-02-27T07:04:04.748898Z",
     "iopub.status.idle": "2026-02-27T07:04:26.280549Z",
     "shell.execute_reply": "2026-02-27T07:04:26.278924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generators: ['tick', 'rocketeval', 'rlcf_direct', 'rlcf_candidate', 'rlcf_candidates_only', 'feedback', 'checkeval', 'interacteval', 'my_gen', 'my_eval']\n",
      "Scorers: ['batch', 'item', 'weighted', 'normalized', 'strict_eval']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pass rate: 100%\n"
     ]
    }
   ],
   "source": [
    "from autochecklist import register_custom_generator, register_custom_scorer, list_generators, list_scorers\n",
    "\n",
    "# Register a custom generator under a name\n",
    "register_custom_generator(\"my_eval\", \"my_generator.md\")\n",
    "\n",
    "# Register a custom scorer under a name\n",
    "register_custom_scorer(\"strict_eval\", \"my_scorer.md\")\n",
    "\n",
    "# Now they appear in the registry\n",
    "print(\"Generators:\", list_generators())\n",
    "print(\"Scorers:\", list_scorers())\n",
    "\n",
    "# Use by name, just like built-in methods\n",
    "pipe = pipeline(\"my_eval\", generator_model=MODEL, scorer=\"strict_eval\")\n",
    "result = pipe(\"Write a haiku about rain\", target=\"Drops fall from the sky\\nPuddles form on empty streets\\nEarth drinks deeply now\")\n",
    "print(f\"\\nPass rate: {result.pass_rate:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scorer Config: Mode, Primary Metric, and More\n",
    "\n",
    "When registering custom scorers or pipelines, you can configure the scorer's behavior — not just the prompt. This gives you the same flexibility as built-in presets like `rlcf_direct` or `rocketeval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:04:26.283622Z",
     "iopub.status.busy": "2026-02-27T07:04:26.283335Z",
     "iopub.status.idle": "2026-02-27T07:04:26.288793Z",
     "shell.execute_reply": "2026-02-27T07:04:26.287574Z"
    }
   },
   "outputs": [],
   "source": "# register_custom_scorer with config kwargs\n# By default, custom scorers use mode=\"batch\". You can configure the full\n# scorer behavior: mode, primary_metric, capture_reasoning.\n# Setting primary_metric=\"normalized\" auto-enables logprobs.\n\nregister_custom_scorer(\n    \"weighted_strict\",\n    \"my_scorer.md\",\n    mode=\"item\",              # evaluate one item per LLM call\n    primary_metric=\"weighted\",  # Score.primary_score uses weighted_score\n)\n\nprint(\"Registered 'weighted_strict' scorer with mode=item, primary_metric=weighted\")\nprint(\"Scorers:\", list_scorers())"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:04:26.291607Z",
     "iopub.status.busy": "2026-02-27T07:04:26.291349Z",
     "iopub.status.idle": "2026-02-27T07:04:26.298664Z",
     "shell.execute_reply": "2026-02-27T07:04:26.297606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline created with scorer mode: item\n",
      "Primary metric: weighted\n"
     ]
    }
   ],
   "source": [
    "# register_custom_pipeline with full scorer config\n",
    "# Instead of just scorer=\"weighted\", specify the exact scorer behavior:\n",
    "from autochecklist import register_custom_pipeline\n",
    "from pathlib import Path\n",
    "\n",
    "# Example 1: with a built-in scorer prompt name (\"rlcf\", \"rocketeval\")\n",
    "register_custom_pipeline(\n",
    "    \"my_weighted_eval\",\n",
    "    generator_prompt=\"Generate yes/no evaluation questions for:\\n\\n{input}\",\n",
    "    scorer_mode=\"item\",              # one item per LLM call\n",
    "    primary_metric=\"weighted\",       # Score.primary_score = weighted_score\n",
    "    scorer_prompt=\"rlcf\",            # built-in scorer prompt name\n",
    ")\n",
    "\n",
    "# Example 2: with a custom scorer prompt file\n",
    "register_custom_pipeline(\n",
    "    \"my_custom_eval\",\n",
    "    generator_prompt=\"Generate yes/no evaluation questions for:\\n\\n{input}\",\n",
    "    scorer_mode=\"batch\",\n",
    "    scorer_prompt=Path(\"my_scorer.md\"),  # reads file at registration time\n",
    "    force=True,  # allow re-registration\n",
    ")\n",
    "\n",
    "# Use it like any built-in preset\n",
    "pipe = pipeline(\"my_weighted_eval\", generator_model=MODEL, scorer_model=MODEL)\n",
    "print(\"Pipeline created with scorer mode:\", pipe.scorer.mode)\n",
    "print(\"Primary metric:\", pipe.scorer.primary_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:04:26.301254Z",
     "iopub.status.busy": "2026-02-27T07:04:26.300990Z",
     "iopub.status.idle": "2026-02-27T07:04:26.308763Z",
     "shell.execute_reply": "2026-02-27T07:04:26.307454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved config:\n",
      "{\n",
      "  \"name\": \"my_weighted_eval\",\n",
      "  \"generator_class\": \"direct\",\n",
      "  \"generator_prompt\": \"Generate yes/no evaluation questions for:\\n\\n{input}\",\n",
      "  \"scorer_mode\": \"item\",\n",
      "  \"scorer_prompt\": \"rlcf\",\n",
      "  \"primary_metric\": \"weighted\",\n",
      "  \"use_logprobs\": false,\n",
      "  \"capture_reasoning\": false\n",
      "}\n",
      "\n",
      "Loaded pipeline: 'my_weighted_eval'\n"
     ]
    }
   ],
   "source": [
    "# Save and load pipeline configs — scorer settings are preserved\n",
    "from autochecklist import save_pipeline_config, load_pipeline_config\n",
    "import json as _json\n",
    "\n",
    "save_pipeline_config(\"my_weighted_eval\", \"my_weighted_eval.json\")\n",
    "\n",
    "# Inspect the saved JSON — scorer config is stored as flat keys\n",
    "with open(\"my_weighted_eval.json\") as f:\n",
    "    config = _json.load(f)\n",
    "print(\"Saved config:\")\n",
    "print(_json.dumps(config, indent=2))\n",
    "\n",
    "# Reload on another machine or in another session\n",
    "loaded_name = load_pipeline_config(\"my_weighted_eval.json\", force=True)\n",
    "print(f\"\\nLoaded pipeline: '{loaded_name}'\")\n",
    "\n",
    "# Clean up\n",
    "import os\n",
    "os.remove(\"my_weighted_eval.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Custom Generator (Advanced — Subclassing)\n",
    "\n",
    "For full control over generation logic, create a Python class:\n",
    "\n",
    "1. Inherit from `InstanceChecklistGenerator` (one checklist per input) or `CorpusChecklistGenerator` (one checklist per dataset)\n",
    "2. Implement `method_name` property and `generate()` method\n",
    "3. Use `@register_generator(\"name\")` to make it discoverable\n",
    "\n",
    "**Available helpers:**\n",
    "- `self._call_model(prompt)` - Call the LLM\n",
    "- `self.model`, `self.temperature` - Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:04:26.311623Z",
     "iopub.status.busy": "2026-02-27T07:04:26.311344Z",
     "iopub.status.idle": "2026-02-27T07:04:26.328637Z",
     "shell.execute_reply": "2026-02-27T07:04:26.327355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleGenerator registered!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Optional, Any\n",
    "\n",
    "from autochecklist import (\n",
    "    InstanceChecklistGenerator,\n",
    "    Checklist,\n",
    "    ChecklistItem,\n",
    "    register_generator,\n",
    ")\n",
    "\n",
    "\n",
    "@register_generator(\"simple\")\n",
    "class SimpleGenerator(InstanceChecklistGenerator):\n",
    "    \"\"\"A minimal generator that creates 3 checklist items from an input.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def method_name(self) -> str:\n",
    "        return \"simple\"\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        input: str,\n",
    "        target: Optional[str] = None,\n",
    "        reference: Optional[str] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Checklist:\n",
    "        # Create the prompt\n",
    "        prompt = f\"\"\"Given this input: \\\"{input}\\\"\n",
    "\n",
    "Generate exactly 3 yes/no questions to evaluate a response.\n",
    "Format each question on its own line, starting with a number:\n",
    "1. <question>\n",
    "2. <question>\n",
    "3. <question>\"\"\"\n",
    "\n",
    "        # Call the LLM (uses self.model, self.temperature automatically)\n",
    "        raw_response = self._call_model(prompt)\n",
    "\n",
    "        # Parse the response into ChecklistItems\n",
    "        items = []\n",
    "        for line in raw_response.strip().split(\"\\n\"):\n",
    "            # Match lines like \"1. Is the response...\" or \"1) Is the response...\"\n",
    "            match = re.match(r\"^\\d+[.)\\s]+(.+)$\", line.strip())\n",
    "            if match:\n",
    "                question = match.group(1).strip()\n",
    "                items.append(ChecklistItem(question=question))\n",
    "\n",
    "        # Return a Checklist\n",
    "        return Checklist(\n",
    "            items=items,\n",
    "            source_method=self.method_name,\n",
    "            generation_level=self.generation_level,  # \"instance\" from base class\n",
    "            input=input,\n",
    "            metadata={\"raw_response\": raw_response},\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"SimpleGenerator registered!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:04:26.331398Z",
     "iopub.status.busy": "2026-02-27T07:04:26.331093Z",
     "iopub.status.idle": "2026-02-27T07:04:26.335898Z",
     "shell.execute_reply": "2026-02-27T07:04:26.334668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available generators: ['tick', 'rocketeval', 'rlcf_direct', 'rlcf_candidate', 'rlcf_candidates_only', 'feedback', 'checkeval', 'interacteval', 'my_gen', 'my_eval', 'my_weighted_eval', 'my_custom_eval', 'simple']\n"
     ]
    }
   ],
   "source": [
    "# Verify it appears in the registry\n",
    "from autochecklist import list_generators\n",
    "\n",
    "print(\"Available generators:\", list_generators())\n",
    "assert \"simple\" in list_generators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:04:26.338976Z",
     "iopub.status.busy": "2026-02-27T07:04:26.338585Z",
     "iopub.status.idle": "2026-02-27T07:04:36.222349Z",
     "shell.execute_reply": "2026-02-27T07:04:36.220895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 items:\n",
      "1. Is the response written as a three-line poem (haiku format)?\n",
      "2. Does the poem evoke autumn through imagery, words, or sensory details?\n",
      "3. Does the poem adhere to the traditional 5-7-5 syllable structure?\n"
     ]
    }
   ],
   "source": [
    "# Use it directly\n",
    "generator = SimpleGenerator(model=MODEL)\n",
    "checklist = generator.generate(input=\"Write a haiku about autumn\")\n",
    "\n",
    "print(f\"Generated {len(checklist)} items:\")\n",
    "print(checklist.to_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:04:36.225594Z",
     "iopub.status.busy": "2026-02-27T07:04:36.225312Z",
     "iopub.status.idle": "2026-02-27T07:05:15.243031Z",
     "shell.execute_reply": "2026-02-27T07:05:15.241662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass rate: 100%\n"
     ]
    }
   ],
   "source": [
    "# Use it via pipeline() - the recommended way\n",
    "from autochecklist import pipeline\n",
    "\n",
    "pipe = pipeline(\"simple\", generator_model=MODEL, scorer_model=MODEL)\n",
    "\n",
    "result = pipe(\n",
    "    input=\"Write a haiku about autumn\",\n",
    "    target=\"Leaves fall gently down\\nCrisp air whispers through the trees\\nNature's final bow\",\n",
    ")\n",
    "\n",
    "print(f\"Pass rate: {result.pass_rate:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Custom Scorer (Advanced — Subclassing)\n",
    "\n",
    "Scorers evaluate targets against checklists. To create one:\n",
    "\n",
    "1. Inherit from `ChecklistScorer`\n",
    "2. Implement `scoring_method` property and `score()` method\n",
    "3. Use `@register_scorer(\"name\")` to make it discoverable\n",
    "\n",
    "**Available helpers:**\n",
    "- `self._call_model(prompt)` - Call the LLM\n",
    "- `score_batch()` - Default batch implementation (override for efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:05:15.246928Z",
     "iopub.status.busy": "2026-02-27T07:05:15.246654Z",
     "iopub.status.idle": "2026-02-27T07:05:15.261960Z",
     "shell.execute_reply": "2026-02-27T07:05:15.260504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StrictScorer registered!\n"
     ]
    }
   ],
   "source": [
    "from autochecklist import (\n",
    "    ChecklistScorer,\n",
    "    Score,\n",
    "    ItemScore,\n",
    "    ChecklistItemAnswer,\n",
    "    register_scorer,\n",
    ")\n",
    "\n",
    "\n",
    "@register_scorer(\"strict\")\n",
    "class StrictScorer(ChecklistScorer):\n",
    "    \"\"\"A scorer that evaluates each item and computes pass rate.\n",
    "    \n",
    "    Uses a simple prompt to determine YES/NO for each checklist item.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def scoring_method(self) -> str:\n",
    "        return \"strict\"\n",
    "\n",
    "    def score(\n",
    "        self,\n",
    "        checklist: Checklist,\n",
    "        target: str,\n",
    "        input: Optional[str] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Score:\n",
    "        item_scores = []\n",
    "\n",
    "        for item in checklist.items:\n",
    "            # Create evaluation prompt\n",
    "            prompt = f\"\"\"Evaluate whether this response satisfies the criterion.\n",
    "\n",
    "Input: {input or checklist.input or 'N/A'}\n",
    "\n",
    "Response: {target}\n",
    "\n",
    "Criterion: {item.question}\n",
    "\n",
    "Answer with only YES or NO.\"\"\"\n",
    "\n",
    "            # Get LLM judgment\n",
    "            raw_answer = self._call_model(prompt).strip().upper()\n",
    "            \n",
    "            # Parse answer\n",
    "            if \"YES\" in raw_answer:\n",
    "                answer = ChecklistItemAnswer.YES\n",
    "            elif \"NO\" in raw_answer:\n",
    "                answer = ChecklistItemAnswer.NO\n",
    "            else:\n",
    "                answer = ChecklistItemAnswer.NA\n",
    "\n",
    "            item_scores.append(ItemScore(item_id=item.id, answer=answer))\n",
    "\n",
    "        # Calculate aggregate score\n",
    "        yes_count = sum(1 for s in item_scores if s.answer == ChecklistItemAnswer.YES)\n",
    "        total = len(item_scores)\n",
    "        pass_rate = yes_count / total if total > 0 else 0.0\n",
    "\n",
    "        return Score(\n",
    "            checklist_id=checklist.id,\n",
    "            item_scores=item_scores,\n",
    "            total_score=pass_rate,\n",
    "            judge_model=self.model,\n",
    "            scoring_method=self.scoring_method,\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"StrictScorer registered!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:05:15.264695Z",
     "iopub.status.busy": "2026-02-27T07:05:15.264426Z",
     "iopub.status.idle": "2026-02-27T07:05:15.269336Z",
     "shell.execute_reply": "2026-02-27T07:05:15.268148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available scorers: ['batch', 'item', 'weighted', 'normalized', 'strict_eval', 'weighted_strict', 'strict']\n"
     ]
    }
   ],
   "source": [
    "# Verify it appears in the registry\n",
    "from autochecklist import list_scorers\n",
    "\n",
    "print(\"Available scorers:\", list_scorers())\n",
    "assert \"strict\" in list_scorers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:05:15.272574Z",
     "iopub.status.busy": "2026-02-27T07:05:15.272317Z",
     "iopub.status.idle": "2026-02-27T07:05:28.626634Z",
     "shell.execute_reply": "2026-02-27T07:05:28.625112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass rate (strict scorer): 100%\n",
      "\n",
      "Item-level results:\n",
      "  YES - Is the poem written in three lines (typical haiku form)?\n",
      "  YES - Does the poem follow the 5-7-5 syllable pattern?\n",
      "  YES - Does the poem clearly evoke autumn through words or imagery?\n"
     ]
    }
   ],
   "source": [
    "# Use it via pipeline\n",
    "pipe = pipeline(\"simple\", generator_model=MODEL, scorer=\"strict\")\n",
    "\n",
    "result = pipe(\n",
    "    input=\"Write a haiku about autumn\",\n",
    "    target=\"Leaves fall gently down\\nCrisp air whispers through the trees\\nNature's final bow\",\n",
    ")\n",
    "\n",
    "print(f\"Pass rate (strict scorer): {result.pass_rate:.0%}\")\n",
    "print(\"\\nItem-level results:\")\n",
    "for item, score in zip(result.checklist.items, result.score.item_scores):\n",
    "    print(f\"  {score.answer.value.upper():3} - {item.question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Custom Refiner (Advanced — Subclassing)\n",
    "\n",
    "Refiners improve checklists by filtering, deduplicating, or transforming items. To create one:\n",
    "\n",
    "1. Inherit from `ChecklistRefiner`\n",
    "2. Implement `refiner_name` property and `refine()` method\n",
    "3. Use `@register_refiner(\"name\")` to make it discoverable\n",
    "\n",
    "**Available helpers:**\n",
    "- `self._call_model(prompt, response_format=...)` - Call LLM with optional JSON schema\n",
    "- `self._create_refined_checklist(original, items, metadata_updates)` - Preserve metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:05:28.629784Z",
     "iopub.status.busy": "2026-02-27T07:05:28.629502Z",
     "iopub.status.idle": "2026-02-27T07:05:28.637492Z",
     "shell.execute_reply": "2026-02-27T07:05:28.636167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LengthFilter registered!\n"
     ]
    }
   ],
   "source": [
    "from autochecklist import ChecklistRefiner, register_refiner\n",
    "\n",
    "\n",
    "@register_refiner(\"length_filter\")\n",
    "class LengthFilter(ChecklistRefiner):\n",
    "    \"\"\"Filter out checklist items that are too short or too long.\n",
    "    \n",
    "    This is a simple refiner that doesn't use LLM calls.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def refiner_name(self) -> str:\n",
    "        return \"length_filter\"\n",
    "\n",
    "    def refine(\n",
    "        self,\n",
    "        checklist: Checklist,\n",
    "        min_length: int = 20,\n",
    "        max_length: int = 200,\n",
    "        **kwargs: Any,\n",
    "    ) -> Checklist:\n",
    "        # Filter items by question length\n",
    "        filtered_items = [\n",
    "            item\n",
    "            for item in checklist.items\n",
    "            if min_length <= len(item.question) <= max_length\n",
    "        ]\n",
    "\n",
    "        # Track what was filtered\n",
    "        removed = len(checklist.items) - len(filtered_items)\n",
    "        metadata_updates = {\n",
    "            \"length_filter_removed\": removed,\n",
    "            \"min_length\": min_length,\n",
    "            \"max_length\": max_length,\n",
    "        }\n",
    "\n",
    "        # Use helper to preserve original metadata\n",
    "        return self._create_refined_checklist(\n",
    "            original=checklist,\n",
    "            items=filtered_items,\n",
    "            metadata_updates=metadata_updates,\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"LengthFilter registered!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:05:28.640171Z",
     "iopub.status.busy": "2026-02-27T07:05:28.639863Z",
     "iopub.status.idle": "2026-02-27T07:05:28.644763Z",
     "shell.execute_reply": "2026-02-27T07:05:28.643391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available refiners: ['deduplicator', 'tagger', 'unit_tester', 'selector', 'length_filter']\n"
     ]
    }
   ],
   "source": [
    "# Verify it appears in the registry\n",
    "from autochecklist import list_refiners\n",
    "\n",
    "print(\"Available refiners:\", list_refiners())\n",
    "assert \"length_filter\" in list_refiners()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:05:28.647282Z",
     "iopub.status.busy": "2026-02-27T07:05:28.647006Z",
     "iopub.status.idle": "2026-02-27T07:05:28.654859Z",
     "shell.execute_reply": "2026-02-27T07:05:28.653524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 4 items\n",
      "  [  6 chars] Short?...\n",
      "  [ 31 chars] Is this a good length question?...\n",
      "  [ 47 chars] Does the response adequately address the topic?...\n",
      "  [250 chars] XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...\n",
      "\n",
      "After: 2 items\n",
      "  [ 31 chars] Is this a good length question?\n",
      "  [ 47 chars] Does the response adequately address the topic?\n",
      "\n",
      "Metadata: {'refined_by': 'length_filter', 'original_count': 4, 'length_filter_removed': 2, 'min_length': 20, 'max_length': 200}\n"
     ]
    }
   ],
   "source": [
    "# Use it directly\n",
    "refiner = LengthFilter()\n",
    "\n",
    "# Create a test checklist with varying question lengths\n",
    "test_checklist = Checklist(\n",
    "    items=[\n",
    "        ChecklistItem(question=\"Short?\"),  # Too short (7 chars)\n",
    "        ChecklistItem(question=\"Is this a good length question?\"),  # OK (31 chars)\n",
    "        ChecklistItem(question=\"Does the response adequately address the topic?\"),  # OK (47 chars)\n",
    "        ChecklistItem(question=\"X\" * 250),  # Too long (250 chars)\n",
    "    ],\n",
    "    source_method=\"test\",\n",
    "    generation_level=\"instance\",\n",
    ")\n",
    "\n",
    "print(f\"Before: {len(test_checklist)} items\")\n",
    "for item in test_checklist.items:\n",
    "    print(f\"  [{len(item.question):3} chars] {item.question[:50]}...\")\n",
    "\n",
    "refined = refiner.refine(test_checklist, min_length=20, max_length=200)\n",
    "\n",
    "print(f\"\\nAfter: {len(refined)} items\")\n",
    "for item in refined.items:\n",
    "    print(f\"  [{len(item.question):3} chars] {item.question[:50]}\")\n",
    "\n",
    "print(f\"\\nMetadata: {refined.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Putting It All Together\n",
    "\n",
    "Custom components work seamlessly with the pipeline API and registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:05:28.657453Z",
     "iopub.status.busy": "2026-02-27T07:05:28.657188Z",
     "iopub.status.idle": "2026-02-27T07:06:07.016065Z",
     "shell.execute_reply": "2026-02-27T07:06:07.014882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline result: 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual result: 33%\n"
     ]
    }
   ],
   "source": [
    "from autochecklist import pipeline, ChecklistPipeline, get_generator, get_scorer, get_refiner\n",
    "\n",
    "# Method 1: Use pipeline() with registered names\n",
    "pipe = pipeline(\"simple\", generator_model=MODEL, scorer=\"strict\")\n",
    "result = pipe(\"Write a poem\", target=\"Roses are red, violets are blue.\")\n",
    "print(f\"Pipeline result: {result.pass_rate:.0%}\")\n",
    "\n",
    "# Method 2: Get classes from registry and instantiate manually\n",
    "gen_cls = get_generator(\"simple\")\n",
    "scorer_cls = get_scorer(\"strict\")\n",
    "\n",
    "generator = gen_cls(model=MODEL)\n",
    "scorer = scorer_cls(model=MODEL)\n",
    "\n",
    "checklist = generator.generate(input=\"Write a poem\")\n",
    "score = scorer.score(checklist, target=\"Roses are red, violets are blue.\")\n",
    "print(f\"Manual result: {score.pass_rate:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Quick Reference\n\n### Easy: `.md` File Approach\n\n| What | How | Example |\n|------|-----|--------|\n| Custom generator | `register_custom_generator(\"name\", \"file.md\")` then `pipeline(\"name\", generator_model=MODEL)` | `.md` with `{input}` |\n| Custom scorer | `register_custom_scorer(\"name\", \"file.md\")` then `pipeline(\"tick\", scorer=\"name\")` | `.md` with `{input}`, `{target}`, `{checklist}` |\n| Custom scorer (configured) | `register_custom_scorer(\"name\", \"file.md\", mode=\"item\", primary_metric=\"weighted\")` | Supports `mode`, `primary_metric`, `capture_reasoning` |\n| Custom pipeline | `register_custom_pipeline(\"name\", generator_prompt=\"...\", scorer_mode=\"item\", primary_metric=\"weighted\")` | Full scorer config via flat kwargs |\n| Both custom | Register both, then `pipeline(\"gen_name\", scorer=\"scorer_name\")` | |\n\n### Advanced: Subclassing\n\n| Component | Base Class | Required Methods | Decorator |\n|-----------|------------|------------------|----------|\n| Generator (instance) | `InstanceChecklistGenerator` | `method_name`, `generate(input, ...)` | `@register_generator(\"name\")` |\n| Generator (corpus) | `CorpusChecklistGenerator` | `method_name`, `generate(inputs, ...)` | `@register_generator(\"name\")` |\n| Scorer | `ChecklistScorer` | `scoring_method`, `score(checklist, target, ...)` | `@register_scorer(\"name\")` |\n| Refiner | `ChecklistRefiner` | `refiner_name`, `refine(checklist, ...)` | `@register_refiner(\"name\")` |\n\n**Return types:**\n- Generators return `Checklist` (with `ChecklistItem` objects)\n- Scorers return `Score` (with `ItemScore` objects and aggregates)\n- Refiners return `Checklist` (refined version)\n\n**For more complex examples**, see the built-in implementations:\n- Generators: `autochecklist/generators/template.py`\n- Scorers: `autochecklist/scorers/base.py`\n- Refiners: `autochecklist/refiners/deduplicator.py`"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}