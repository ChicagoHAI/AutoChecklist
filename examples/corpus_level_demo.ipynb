{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Corpus-Level Generators Demo\n",
    "\n",
    "This notebook demonstrates the **corpus-level** checklist generators in `autochecklist`.\n",
    "\n",
    "Corpus-level generators create **one checklist for an entire dataset/task** (vs. instance-level which creates one per input).\n",
    "\n",
    "**Methods covered:**\n",
    "1. **InductiveGenerator** - From feedback comments with refinement pipeline\n",
    "2. **DeductiveGenerator** (CheckEval) - From dimension definitions with augmentation modes and optional filtering\n",
    "3. **InteractiveGenerator** (InteractEval) - From think-aloud attributes via 5-stage pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T05:53:47.129859Z",
     "iopub.status.busy": "2026-02-27T05:53:47.129722Z",
     "iopub.status.idle": "2026-02-27T05:53:47.142611Z",
     "shell.execute_reply": "2026-02-27T05:53:47.141340Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# OpenRouter is the default provider. For other providers, see pipeline_demo.ipynb\n",
    "# Supported: OpenRouter, OpenAI (direct), vLLM (server or offline)\n",
    "assert os.getenv(\"OPENROUTER_API_KEY\"), \"Please set OPENROUTER_API_KEY in .env file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T05:53:47.145612Z",
     "iopub.status.busy": "2026-02-27T05:53:47.145322Z",
     "iopub.status.idle": "2026-02-27T05:53:47.294027Z",
     "shell.execute_reply": "2026-02-27T05:53:47.293115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus-level generators: ['InductiveGenerator', 'DeductiveGenerator', 'InteractiveGenerator']\n"
     ]
    }
   ],
   "source": [
    "# Import corpus-level generators\n",
    "from autochecklist import (\n",
    "    # Corpus-level generators\n",
    "    InductiveGenerator,\n",
    "    DeductiveGenerator, AugmentationMode,\n",
    "    InteractiveGenerator,\n",
    "    # Input models\n",
    "    DeductiveInput, InteractiveInput,\n",
    "    # Scorer (can be used with any checklist)\n",
    "    ChecklistScorer,\n",
    ")\n",
    "\n",
    "print(\"Corpus-level generators:\", [\"InductiveGenerator\", \"DeductiveGenerator\", \"InteractiveGenerator\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T05:53:47.296277Z",
     "iopub.status.busy": "2026-02-27T05:53:47.296129Z",
     "iopub.status.idle": "2026-02-27T05:53:47.298910Z",
     "shell.execute_reply": "2026-02-27T05:53:47.298204Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL = \"openai/gpt-5-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. InductiveGenerator\n",
    "\n",
    "**InductiveGenerator** creates checklists from feedback comments (e.g., reviewer feedback, user complaints, quality notes).\n",
    "\n",
    "**Pipeline:**\n",
    "1. **Generate**: LLM creates questions from feedback batches\n",
    "2. **Deduplicate**: Merge semantically similar questions (embedding similarity)\n",
    "3. **Tag**: Filter by applicability and specificity\n",
    "4. **Select**: Beam search for diverse subset\n",
    "\n",
    "**Use case**: You have collected feedback about responses and want to create a checklist that addresses the common issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T05:53:47.300784Z",
     "iopub.status.busy": "2026-02-27T05:53:47.300649Z",
     "iopub.status.idle": "2026-02-27T05:53:47.304563Z",
     "shell.execute_reply": "2026-02-27T05:53:47.303684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 10 feedback comments:\n",
      "  1. The explanation was too long and hard to follow.\n",
      "  2. Good use of examples to illustrate concepts.\n",
      "  3. Missing important details about edge cases.\n",
      "  4. The code snippets were helpful and clear.\n",
      "  5. Structure was confusing - hard to find what I needed.\n",
      "  6. Excellent step-by-step instructions.\n",
      "  7. Some technical terms weren't explained.\n",
      "  8. The documentation was outdated in some sections.\n",
      "  9. Would benefit from more visual diagrams.\n",
      "  10. The examples don't cover all use cases.\n"
     ]
    }
   ],
   "source": [
    "# Sample feedback comments about technical documentation\n",
    "feedback = [\n",
    "    \"The explanation was too long and hard to follow.\",\n",
    "    \"Good use of examples to illustrate concepts.\",\n",
    "    \"Missing important details about edge cases.\",\n",
    "    \"The code snippets were helpful and clear.\",\n",
    "    \"Structure was confusing - hard to find what I needed.\",\n",
    "    \"Excellent step-by-step instructions.\",\n",
    "    \"Some technical terms weren't explained.\",\n",
    "    \"The documentation was outdated in some sections.\",\n",
    "    \"Would benefit from more visual diagrams.\",\n",
    "    \"The examples don't cover all use cases.\",\n",
    "]\n",
    "\n",
    "print(f\"Collected {len(feedback)} feedback comments:\")\n",
    "for i, f in enumerate(feedback, 1):\n",
    "    print(f\"  {i}. {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T05:53:47.306679Z",
     "iopub.status.busy": "2026-02-27T05:53:47.306484Z",
     "iopub.status.idle": "2026-02-27T05:54:11.621948Z",
     "shell.execute_reply": "2026-02-27T05:54:11.621103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[InductiveGenerator] Generated 7 raw questions from 10 observations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[InductiveGenerator] Deduplication: 7 → 7 questions (0 clusters merged)\n",
      "[InductiveGenerator] Final checklist: 7 questions\n",
      "Generated 7 questions from 10 feedback comments:\n",
      "\n",
      "  1. Is the content concise, well-organized, and easy to follow so readers can quickly find what they need?\n",
      "  2. Are examples provided that clearly illustrate the concepts?\n",
      "  3. Do the examples and coverage include common and important use cases and edge cases?\n",
      "  4. Are any code snippets included clear, correct, and directly usable by readers?\n",
      "  5. Are technical terms and jargon defined or linked to clear explanations?\n",
      "  6. Is the documentation up-to-date and accurate for current versions, APIs, or standards?\n",
      "  7. Does the documentation include helpful diagrams or visuals where they improve understanding?\n"
     ]
    }
   ],
   "source": [
    "# Generate checklist from feedback\n",
    "feedback_gen = InductiveGenerator(\n",
    "    model=MODEL,\n",
    "    dedup_threshold=0.85,  # Merge questions with >85% similarity\n",
    "    max_questions=10,      # Limit final checklist size\n",
    ")\n",
    "\n",
    "checklist = feedback_gen.generate(\n",
    "    observations=feedback,\n",
    "    domain=\"technical documentation\",\n",
    "    skip_selection=True,\n",
    "    skip_tagging=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(checklist.items)} questions from {len(feedback)} feedback comments:\")\n",
    "print()\n",
    "for i, item in enumerate(checklist.items, 1):\n",
    "    print(f\"  {i}. {item.question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T05:54:11.623963Z",
     "iopub.status.busy": "2026-02-27T05:54:11.623709Z",
     "iopub.status.idle": "2026-02-27T05:54:11.626617Z",
     "shell.execute_reply": "2026-02-27T05:54:11.626127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checklist metadata:\n",
      "  Source method: feedback\n",
      "  Generation level: corpus\n",
      "  Observation count: 10\n"
     ]
    }
   ],
   "source": [
    "# Check metadata\n",
    "print(\"Checklist metadata:\")\n",
    "print(f\"  Source method: {checklist.source_method}\")\n",
    "print(f\"  Generation level: {checklist.generation_level}\")\n",
    "print(f\"  Observation count: {checklist.metadata.get('observation_count')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T05:54:11.628270Z",
     "iopub.status.busy": "2026-02-27T05:54:11.628136Z",
     "iopub.status.idle": "2026-02-27T05:54:22.907074Z",
     "shell.execute_reply": "2026-02-27T05:54:22.905753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentation score: 0%\n",
      "\n",
      "Item breakdown:\n",
      "  [NO ] Is the content concise, well-organized, and easy to follow s...\n",
      "  [NO ] Are examples provided that clearly illustrate the concepts?...\n",
      "  [NO ] Do the examples and coverage include common and important us...\n",
      "  [NO ] Are any code snippets included clear, correct, and directly ...\n",
      "  [NO ] Are technical terms and jargon defined or linked to clear ex...\n",
      "  [NO ] Is the documentation up-to-date and accurate for current ver...\n",
      "  [NO ] Does the documentation include helpful diagrams or visuals w...\n"
     ]
    }
   ],
   "source": [
    "# Use the checklist to score documentation\n",
    "scorer = ChecklistScorer(mode=\"batch\", model=MODEL)\n",
    "\n",
    "sample_doc = \"\"\"\n",
    "# How to Use the API\n",
    "\n",
    "This guide explains how to make API calls.\n",
    "\n",
    "## Step 1: Authentication\n",
    "First, get your API key from the dashboard.\n",
    "\n",
    "## Step 2: Make a Request\n",
    "```python\n",
    "response = client.get('/users')\n",
    "```\n",
    "\n",
    "The response contains user data.\n",
    "\"\"\"\n",
    "\n",
    "score = scorer.score(checklist, target=sample_doc)\n",
    "\n",
    "print(f\"Documentation score: {score.pass_rate:.0%}\")\n",
    "print(\"\\nItem breakdown:\")\n",
    "for item, item_score in zip(checklist.items, score.item_scores):\n",
    "    print(f\"  [{item_score.answer.name:3}] {item.question[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. DeductiveGenerator (CheckEval)\n",
    "\n",
    "**DeductiveGenerator** generates checklists from evaluation dimension definitions.\n",
    "\n",
    "**Four augmentation modes:**\n",
    "- **seed**: Minimal questions (1-3 per sub-dimension)\n",
    "- **elaboration**: Detailed questions with more specificity (5+ per sub-dimension)\n",
    "- **diversification**: Alternative framings for each criterion\n",
    "- **combined**: Paper-faithful pipeline — generates seeds, then runs both elaboration and diversification, then merges all three pools\n",
    "\n",
    "**Use case**: You have defined evaluation criteria (dimensions) and want to generate binary yes/no questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T05:54:22.910625Z",
     "iopub.status.busy": "2026-02-27T05:54:22.910330Z",
     "iopub.status.idle": "2026-02-27T05:54:22.916719Z",
     "shell.execute_reply": "2026-02-27T05:54:22.915387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dimensions:\n",
      "\n",
      "  COHERENCE\n",
      "    Definition: The response should maintain logical flow and consistency throughout.\n",
      "    Sub-dimensions: ['Logical Flow', 'Consistency', 'Organization']\n",
      "\n",
      "  CLARITY\n",
      "    Definition: The response should be clear and easy to understand.\n",
      "    Sub-dimensions: ['Language Clarity', 'Structure']\n"
     ]
    }
   ],
   "source": [
    "# Define evaluation dimensions\n",
    "dimensions = [\n",
    "    DeductiveInput(\n",
    "        name=\"coherence\",\n",
    "        definition=\"The response should maintain logical flow and consistency throughout.\",\n",
    "        sub_dimensions=[\"Logical Flow\", \"Consistency\", \"Organization\"],\n",
    "    ),\n",
    "    DeductiveInput(\n",
    "        name=\"clarity\",\n",
    "        definition=\"The response should be clear and easy to understand.\",\n",
    "        sub_dimensions=[\"Language Clarity\", \"Structure\"],\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Evaluation dimensions:\")\n",
    "for dim in dimensions:\n",
    "    print(f\"\\n  {dim.name.upper()}\")\n",
    "    print(f\"    Definition: {dim.definition}\")\n",
    "    print(f\"    Sub-dimensions: {dim.sub_dimensions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T05:54:22.919545Z",
     "iopub.status.busy": "2026-02-27T05:54:22.919236Z",
     "iopub.status.idle": "2026-02-27T05:54:44.685066Z",
     "shell.execute_reply": "2026-02-27T05:54:44.684210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED mode: 10 questions\n",
      "\n",
      "  [coherence] Does the response present ideas in a clear, progressive sequence?\n",
      "  [coherence] Are transitions between points smooth and easy to follow?\n",
      "  [coherence] Is the information presented throughout the response free of contradictions?\n",
      "  [coherence] Does the response use consistent terminology and facts across all sections?\n",
      "  [coherence] Is the response structured with a clear introduction, body, and conclusion?\n",
      "  [coherence] Can the reader quickly identify the main point or purpose of the response?\n",
      "  [clarity] Are the sentences grammatically correct?\n",
      "  [clarity] Is the vocabulary and terminology appropriate and understandable for the intended audience?\n",
      "  [clarity] Is the information organized in a logical sequence?\n",
      "  [clarity] Does each paragraph or section focus on a single main idea?\n"
     ]
    }
   ],
   "source": [
    "# Generate with SEED mode (minimal questions)\n",
    "checkeval_seed = DeductiveGenerator(\n",
    "    model=MODEL,\n",
    "    augmentation_mode=AugmentationMode.SEED,\n",
    ")\n",
    "\n",
    "seed_checklist = checkeval_seed.generate(dimensions=dimensions)\n",
    "\n",
    "print(f\"SEED mode: {len(seed_checklist.items)} questions\")\n",
    "print()\n",
    "for item in seed_checklist.items:\n",
    "    print(f\"  [{item.category}] {item.question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T05:54:44.686779Z",
     "iopub.status.busy": "2026-02-27T05:54:44.686640Z",
     "iopub.status.idle": "2026-02-27T05:55:12.474028Z",
     "shell.execute_reply": "2026-02-27T05:55:12.472897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELABORATION mode: 25 questions\n",
      "\n",
      "Sample questions:\n",
      "  [coherence] Does the response present ideas in a clear, sequential order?\n",
      "  [coherence] Is each sentence or paragraph logically connected to the one before it?\n",
      "  [coherence] Are transitions used to link ideas between sentences or paragraphs?\n",
      "  [coherence] Can the reader trace a clear progression from the introduction to the conclusion?\n",
      "  [coherence] Has the response avoided introducing unrelated points that disrupt the flow?\n",
      "  [coherence] Does the response use the same terms consistently for the same concepts?\n",
      "  ... and 19 more\n"
     ]
    }
   ],
   "source": [
    "# Generate with ELABORATION mode (more detailed questions)\n",
    "checkeval_elab = DeductiveGenerator(\n",
    "    model=MODEL,\n",
    "    augmentation_mode=AugmentationMode.ELABORATION,\n",
    ")\n",
    "\n",
    "elab_checklist = checkeval_elab.generate(dimensions=dimensions)\n",
    "\n",
    "print(f\"ELABORATION mode: {len(elab_checklist.items)} questions\")\n",
    "print()\n",
    "print(\"Sample questions:\")\n",
    "for item in elab_checklist.items[:6]:\n",
    "    print(f\"  [{item.category}] {item.question}\")\n",
    "if len(elab_checklist.items) > 6:\n",
    "    print(f\"  ... and {len(elab_checklist.items) - 6} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T05:55:12.477131Z",
     "iopub.status.busy": "2026-02-27T05:55:12.476848Z",
     "iopub.status.idle": "2026-02-27T05:55:48.322443Z",
     "shell.execute_reply": "2026-02-27T05:55:48.321309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIVERSIFICATION mode: 20 questions\n",
      "\n",
      "Sample questions:\n",
      "  [coherence] Does the response present ideas in a clear, progressive sequence?\n",
      "  [coherence] Are transitions between sentences and paragraphs smooth and connecting ideas?\n",
      "  [coherence] Can the reader follow the argument without encountering abrupt, unexplained jumps?\n",
      "  [coherence] Does each sentence logically follow from the previous one?\n",
      "  [coherence] Is the information presented internally consistent without contradictions?\n",
      "  [coherence] Does the response use the same factual claims throughout without changing them?\n"
     ]
    }
   ],
   "source": [
    "# Generate with DIVERSIFICATION mode (alternative framings)\n",
    "checkeval_div = DeductiveGenerator(\n",
    "    model=MODEL,\n",
    "    augmentation_mode=AugmentationMode.DIVERSIFICATION,\n",
    ")\n",
    "\n",
    "div_checklist = checkeval_div.generate(dimensions=dimensions)\n",
    "\n",
    "print(f\"DIVERSIFICATION mode: {len(div_checklist.items)} questions\")\n",
    "print()\n",
    "print(\"Sample questions:\")\n",
    "for item in div_checklist.items[:6]:\n",
    "    print(f\"  [{item.category}] {item.question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T05:55:48.324344Z",
     "iopub.status.busy": "2026-02-27T05:55:48.324166Z",
     "iopub.status.idle": "2026-02-27T05:55:48.327946Z",
     "shell.execute_reply": "2026-02-27T05:55:48.327108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of augmentation modes:\n",
      "  SEED:            10 questions\n",
      "  ELABORATION:     25 questions\n",
      "  DIVERSIFICATION: 20 questions\n",
      "  COMBINED:        (see next cell)\n"
     ]
    }
   ],
   "source": [
    "# Compare modes\n",
    "print(\"Comparison of augmentation modes:\")\n",
    "print(f\"  SEED:            {len(seed_checklist.items)} questions\")\n",
    "print(f\"  ELABORATION:     {len(elab_checklist.items)} questions\")\n",
    "print(f\"  DIVERSIFICATION: {len(div_checklist.items)} questions\")\n",
    "print(f\"  COMBINED:        (see next cell)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "g15zqmd645p",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T05:55:48.329523Z",
     "iopub.status.busy": "2026-02-27T05:55:48.329358Z",
     "iopub.status.idle": "2026-02-27T05:57:42.828246Z",
     "shell.execute_reply": "2026-02-27T05:57:42.827167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DeductiveGenerator] Generated 61 questions from 2 dimensions\n",
      "[DeductiveGenerator] Final checklist: 61 questions\n",
      "\n",
      "COMBINED mode: 61 questions\n",
      "\n",
      "Sample questions (with augmentation source):\n",
      "  [coherence] (seed) Does the response present points in an order that builds logically on previous points?\n",
      "  [coherence] (seed) Is each paragraph or sentence connected to the previous one without abrupt topic jumps?\n",
      "  [coherence] (seed) Does the response use key terms and facts consistently without contradicting earlier statements?\n",
      "  [coherence] (seed) Is the author's stance or main conclusion maintained consistently throughout the response?\n",
      "  [coherence] (seed) Has the response been organized into clear sections or distinct paragraphs that separate ideas?\n",
      "  [coherence] (seed) Can the main idea or purpose of the response be identified quickly from its structure?\n",
      "  [coherence] (elaboration) Does the response use explicit transitional phrases to signal shifts or links between points?\n",
      "  [coherence] (elaboration) Does the response move from general ideas to specific details (or vice versa) in a way that aids understanding?\n",
      "  ... and 53 more\n"
     ]
    }
   ],
   "source": [
    "# Generate with COMBINED mode (paper-faithful: seed → elaborate + diversify → merge)\n",
    "checkeval_combined = DeductiveGenerator(\n",
    "    model=MODEL,\n",
    "    augmentation_mode=AugmentationMode.COMBINED,\n",
    ")\n",
    "\n",
    "combined_checklist = checkeval_combined.generate(dimensions=dimensions, verbose=True)\n",
    "\n",
    "print(f\"\\nCOMBINED mode: {len(combined_checklist.items)} questions\")\n",
    "print()\n",
    "print(\"Sample questions (with augmentation source):\")\n",
    "for item in combined_checklist.items[:8]:\n",
    "    source = item.metadata.get(\"augmentation_source\", \"seed\") if item.metadata else \"seed\"\n",
    "    print(f\"  [{item.category}] ({source}) {item.question}\")\n",
    "if len(combined_checklist.items) > 8:\n",
    "    print(f\"  ... and {len(combined_checklist.items) - 8} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i4gbix3nsg",
   "metadata": {},
   "source": [
    "### DeductiveGenerator with Filtering Pipeline\n",
    "\n",
    "DeductiveGenerator can optionally apply the CheckEval paper's **3-stage filtering pipeline**:\n",
    "1. **Alignment**: Ensures \"yes\" indicates higher quality\n",
    "2. **Dimension Consistency**: Questions match dimension definitions  \n",
    "3. **Redundancy Removal**: Eliminates overlapping questions\n",
    "\n",
    "Use `apply_filtering=True` to enable, and `verbose=True` to see progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "yrzvnlq4vnd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T05:57:42.831790Z",
     "iopub.status.busy": "2026-02-27T05:57:42.831511Z",
     "iopub.status.idle": "2026-02-27T05:58:12.493216Z",
     "shell.execute_reply": "2026-02-27T05:58:12.491973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DeductiveGenerator] Generated 25 questions from 2 dimensions\n",
      "[DeductiveGenerator] Final checklist: 25 questions\n",
      "\n",
      "Final filtered checklist: 25 questions\n",
      "\n",
      "Sample questions after filtering:\n",
      "  [coherence] Does the response progress in a clear, logical order from one idea to the next?\n",
      "  [coherence] Is each sentence or paragraph linked to the previous one without abrupt jumps?\n",
      "  [coherence] Does the response present premises that directly support its main conclusion?\n",
      "  [coherence] Can a reader follow the sequence of ideas without needing to re-read earlier parts?\n",
      "  [coherence] Has the response avoided introducing unrelated information that breaks the flow?\n"
     ]
    }
   ],
   "source": [
    "# Generate with filtering enabled and verbose output\n",
    "checkeval_filtered = DeductiveGenerator(\n",
    "    model=MODEL,\n",
    "    augmentation_mode=AugmentationMode.ELABORATION,\n",
    ")\n",
    "\n",
    "# Use dimensions from earlier\n",
    "filtered_checklist = checkeval_filtered.generate(\n",
    "    dimensions=dimensions,\n",
    "    # apply_filtering=True,  # Enable the 3-stage filtering pipeline\n",
    "    verbose=True,          # Print progress at each stage\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal filtered checklist: {len(filtered_checklist.items)} questions\")\n",
    "print(\"\\nSample questions after filtering:\")\n",
    "for item in filtered_checklist.items[:5]:\n",
    "    print(f\"  [{item.category}] {item.question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T05:58:12.495986Z",
     "iopub.status.busy": "2026-02-27T05:58:12.495743Z",
     "iopub.status.idle": "2026-02-27T05:58:12.510304Z",
     "shell.execute_reply": "2026-02-27T05:58:12.509201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom seed questions (no LLM): 5 items\n",
      "  - Does the response flow logically?\n",
      "  - Is the response internally consistent?\n",
      "  - Is the response well-organized?\n",
      "  - Is the language clear and unambiguous?\n",
      "  - Is the structure easy to follow?\n"
     ]
    }
   ],
   "source": [
    "# Use pre-defined seed questions (skip LLM generation)\n",
    "seed_questions = {\n",
    "    \"coherence\": {\n",
    "        \"Logical Flow\": [\"Does the response flow logically?\"],\n",
    "        \"Consistency\": [\"Is the response internally consistent?\"],\n",
    "        \"Organization\": [\"Is the response well-organized?\"],\n",
    "    },\n",
    "    \"clarity\": {\n",
    "        \"Language Clarity\": [\"Is the language clear and unambiguous?\"],\n",
    "        \"Structure\": [\"Is the structure easy to follow?\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "checkeval = DeductiveGenerator(model=MODEL)\n",
    "custom_checklist = checkeval.generate(\n",
    "    dimensions=dimensions,\n",
    "    seed_questions=seed_questions,\n",
    "    augment=False,  # Use seed questions directly, no LLM augmentation\n",
    ")\n",
    "\n",
    "print(f\"Custom seed questions (no LLM): {len(custom_checklist.items)} items\")\n",
    "for item in custom_checklist.items:\n",
    "    print(f\"  - {item.question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. InteractiveGenerator (InteractEval)\n",
    "\n",
    "**InteractiveGenerator** generates checklists from think-aloud attributes through a **5-stage pipeline**:\n",
    "\n",
    "1. **Component Extraction**: Find recurring themes from attributes (max 5)\n",
    "2. **Attributes Clustering**: Group attributes under components\n",
    "3. **Key Question Generation**: 1 yes/no question per component\n",
    "4. **Sub-Question Generation**: 2-3 sub-questions per key question\n",
    "5. **Question Validation**: Refine and validate final checklist\n",
    "\n",
    "**Think-aloud attributes** come from human evaluators or LLMs reading samples with rubrics and noting what they look for.\n",
    "\n",
    "**Use case**: You have evaluation considerations from human/LLM think-aloud sessions and want to structure them into a checklist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T05:58:12.512619Z",
     "iopub.status.busy": "2026-02-27T05:58:12.512442Z",
     "iopub.status.idle": "2026-02-27T05:58:12.517496Z",
     "shell.execute_reply": "2026-02-27T05:58:12.516538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Think-aloud attributes (10 items):\n",
      "  Source: human_llm\n",
      "  Dimension: coherence\n",
      "\n",
      "  - Check if the summary maintains logical flow between sentences\n",
      "  - Ensure ideas connect smoothly without abrupt transitions\n",
      "  - Assess if the structure is well-organized with clear sections\n",
      "  - Verify the summary avoids jumping between unrelated topics\n",
      "  - Look for consistent point of view throughout the text\n",
      "  - Check that main points are clearly and concisely presented\n",
      "  - Ensure the summary builds to a coherent conclusion\n",
      "  - Verify there is a clear beginning, middle, and end\n",
      "  - Check for effective use of transition words\n",
      "  - Assess if the summary maintains a consistent narrative voice\n"
     ]
    }
   ],
   "source": [
    "# Think-aloud attributes from evaluation sessions\n",
    "# These are considerations that evaluators noted when assessing summaries\n",
    "attributes = InteractiveInput(\n",
    "    source=\"human_llm\",  # Combined from human and LLM sources\n",
    "    dimension=\"coherence\",\n",
    "    attributes=[\n",
    "        \"Check if the summary maintains logical flow between sentences\",\n",
    "        \"Ensure ideas connect smoothly without abrupt transitions\",\n",
    "        \"Assess if the structure is well-organized with clear sections\",\n",
    "        \"Verify the summary avoids jumping between unrelated topics\",\n",
    "        \"Look for consistent point of view throughout the text\",\n",
    "        \"Check that main points are clearly and concisely presented\",\n",
    "        \"Ensure the summary builds to a coherent conclusion\",\n",
    "        \"Verify there is a clear beginning, middle, and end\",\n",
    "        \"Check for effective use of transition words\",\n",
    "        \"Assess if the summary maintains a consistent narrative voice\",\n",
    "    ],\n",
    "    sample_context=\"Summary evaluation for news articles\",\n",
    ")\n",
    "\n",
    "print(f\"Think-aloud attributes ({len(attributes.attributes)} items):\")\n",
    "print(f\"  Source: {attributes.source}\")\n",
    "print(f\"  Dimension: {attributes.dimension}\")\n",
    "print()\n",
    "for attr in attributes.attributes:\n",
    "    print(f\"  - {attr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T05:58:12.519643Z",
     "iopub.status.busy": "2026-02-27T05:58:12.519474Z",
     "iopub.status.idle": "2026-02-27T05:58:12.523093Z",
     "shell.execute_reply": "2026-02-27T05:58:12.522212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rubric:\n",
      "\n",
      "Coherence measures the collective quality of all sentences. \n",
      "The summary should be well-structured and well-organized. \n",
      "The summary should not just be a heap of related information, \n",
      "but should build from sentence to sentence to a coherent body of information about a topic.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the rubric for the dimension\n",
    "rubric = \"\"\"\n",
    "Coherence measures the collective quality of all sentences. \n",
    "The summary should be well-structured and well-organized. \n",
    "The summary should not just be a heap of related information, \n",
    "but should build from sentence to sentence to a coherent body of information about a topic.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Rubric:\")\n",
    "print(rubric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T05:58:12.525315Z",
     "iopub.status.busy": "2026-02-27T05:58:12.525146Z",
     "iopub.status.idle": "2026-02-27T05:59:27.445240Z",
     "shell.execute_reply": "2026-02-27T05:59:27.443917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 13 questions from 10 attributes:\n",
      "\n",
      "  1. Is there a clear introductory sentence that establishes the topic and purpose?\n",
      "  2. Does the text maintain a single main topic from start to finish?\n",
      "  3. Are paragraphs organized so each focuses on a single main point?\n",
      "  4. Do paragraphs appear in a logical order (for example: context → key points → conclusion)?\n",
      "  5. Do sentences within each paragraph build logically on the previous sentence?\n",
      "  6. Are clear topic sentences or section cues used to signal each paragraph's main point?\n",
      "  7. Are transition words or phrases used to link sentences and paragraphs where needed?\n",
      "  8. Do pronouns and other referential terms clearly and consistently refer back to earlier concepts?\n",
      "  9. Are key terms or repeated words used consistently to reinforce connections between ideas?\n",
      "  10. Are individual sentences clearly relevant to advancing or supporting the main topic (no off-topic content)?\n",
      "  11. Does the final sentence provide a concise conclusion or clear closing that follows from the preceding content?\n",
      "  12. Is the narrative voice (tone and perspective) consistent throughout the text?\n",
      "  13. Is the overall sequence of sentences and paragraphs coherent and easy to follow?\n"
     ]
    }
   ],
   "source": [
    "# Generate checklist via 5-stage pipeline\n",
    "interacteval = InteractiveGenerator(\n",
    "    model=MODEL,\n",
    "    max_components=4,  # Limit to 4 main themes\n",
    ")\n",
    "\n",
    "checklist = interacteval.generate(\n",
    "    inputs=[attributes],\n",
    "    rubric=rubric,\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(checklist.items)} questions from {len(attributes.attributes)} attributes:\")\n",
    "print()\n",
    "for i, item in enumerate(checklist.items, 1):\n",
    "    print(f\"  {i}. {item.question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T05:59:27.447542Z",
     "iopub.status.busy": "2026-02-27T05:59:27.447329Z",
     "iopub.status.idle": "2026-02-27T05:59:27.451941Z",
     "shell.execute_reply": "2026-02-27T05:59:27.451091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checklist metadata:\n",
      "  Source method: interacteval\n",
      "  Generation level: corpus\n",
      "  Dimension: coherence\n",
      "  Attribute count: 10\n",
      "  Source: human_llm\n"
     ]
    }
   ],
   "source": [
    "# Check metadata\n",
    "print(\"Checklist metadata:\")\n",
    "print(f\"  Source method: {checklist.source_method}\")\n",
    "print(f\"  Generation level: {checklist.generation_level}\")\n",
    "print(f\"  Dimension: {checklist.metadata.get('dimension')}\")\n",
    "print(f\"  Attribute count: {checklist.metadata.get('attribute_count')}\")\n",
    "print(f\"  Source: {checklist.metadata.get('source')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T05:59:27.453763Z",
     "iopub.status.busy": "2026-02-27T05:59:27.453578Z",
     "iopub.status.idle": "2026-02-27T06:00:28.679683Z",
     "shell.execute_reply": "2026-02-27T06:00:28.678493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined checklist from human + LLM sources:\n",
      "  Total attributes: 6\n",
      "  Source: human_llm\n",
      "  Questions generated: 11\n",
      "\n",
      "  1. Does the text focus on a single clear main idea or thesis?\n",
      "  2. Is there a clear opening or topic sentence that frames the whole summary?\n",
      "  3. Are all sentences directly relevant to and supportive of that main idea?\n",
      "  4. Do sentences and paragraphs build logically so each one advances the prior content?\n",
      "  5. Is terminology and concept usage consistent across sentences?\n",
      "  6. Are factual statements and claims internally consistent with one another?\n",
      "  7. Are causal and comparative relationships presented clearly and without contradiction?\n",
      "  8. Does the overall sequence follow a clear organizational pattern (for example: background → details → conclusion)?\n",
      "  9. Are explicit transitions or referencing used to signal relationships and maintain continuity between sentences and paragraphs?\n",
      "  10. Are paragraph breaks placed where a new subtopic or logical step begins and do they link smoothly from the previous paragraph?\n",
      "  11. Can a reader anticipate the next logical point based on the current progression?\n"
     ]
    }
   ],
   "source": [
    "# Combine multiple sources (human + LLM)\n",
    "# In practice, you might have separate think-aloud sessions from humans and LLMs\n",
    "\n",
    "human_attrs = InteractiveInput(\n",
    "    source=\"human\",\n",
    "    dimension=\"coherence\",\n",
    "    attributes=[\n",
    "        \"Does it make sense as a whole?\",\n",
    "        \"Can I follow the argument easily?\",\n",
    "        \"Are the ideas connected?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "llm_attrs = InteractiveInput(\n",
    "    source=\"llm\",\n",
    "    dimension=\"coherence\",\n",
    "    attributes=[\n",
    "        \"Check for logical consistency between statements\",\n",
    "        \"Verify smooth transitions between paragraphs\",\n",
    "        \"Assess narrative structure and flow\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "combined_checklist = interacteval.generate(\n",
    "    inputs=[human_attrs, llm_attrs],  # Combine both sources\n",
    "    rubric=rubric,\n",
    ")\n",
    "\n",
    "print(f\"Combined checklist from human + LLM sources:\")\n",
    "print(f\"  Total attributes: {combined_checklist.metadata.get('attribute_count')}\")\n",
    "print(f\"  Source: {combined_checklist.metadata.get('source')}\")\n",
    "print(f\"  Questions generated: {len(combined_checklist.items)}\")\n",
    "print()\n",
    "for i, item in enumerate(combined_checklist.items, 1):\n",
    "    print(f\"  {i}. {item.question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:00:28.683153Z",
     "iopub.status.busy": "2026-02-27T06:00:28.682853Z",
     "iopub.status.idle": "2026-02-27T06:00:54.228010Z",
     "shell.execute_reply": "2026-02-27T06:00:54.226183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "\n",
      "The city council approved a new budget yesterday, allocating increased funds \n",
      "for public transportation and education. The decision follows months of public \n",
      "consultation and reflects community priorities. Mayor Johnson praised the \n",
      "collaborative process, noting that the budget addresses both immediate needs \n",
      "and long-term infrastructure goals. Implementation begins next fiscal year.\n",
      "\n",
      "\n",
      "Coherence score: 100%\n"
     ]
    }
   ],
   "source": [
    "# Use the checklist to score a summary\n",
    "scorer = ChecklistScorer(mode=\"batch\", model=MODEL)\n",
    "\n",
    "good_summary = \"\"\"\n",
    "The city council approved a new budget yesterday, allocating increased funds \n",
    "for public transportation and education. The decision follows months of public \n",
    "consultation and reflects community priorities. Mayor Johnson praised the \n",
    "collaborative process, noting that the budget addresses both immediate needs \n",
    "and long-term infrastructure goals. Implementation begins next fiscal year.\n",
    "\"\"\"\n",
    "\n",
    "score = scorer.score(checklist, target=good_summary)\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(good_summary)\n",
    "print(f\"\\nCoherence score: {score.pass_rate:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Generator | Input | Pipeline | Use Case |\n",
    "|-----------|-------|----------|----------|\n",
    "| **InductiveGenerator** | Feedback comments | Generate → Deduplicate → Tag → Select | From user/reviewer feedback |\n",
    "| **DeductiveGenerator** | Dimension definitions | LLM generation with augmentation modes | From evaluation criteria |\n",
    "| **InteractiveGenerator** | Think-aloud attributes | 5-stage refinement pipeline | From evaluation sessions |\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "- **InductiveGenerator**: Best when you have unstructured feedback and want to synthesize it into evaluation criteria\n",
    "- **DeductiveGenerator**: Best when you have well-defined dimensions and want to expand them into detailed questions\n",
    "- **InteractiveGenerator**: Best when you have evaluation considerations from think-aloud sessions and want to organize them\n",
    "\n",
    "### All checklists can be used with any scorer config\n",
    "\n",
    "Once generated, corpus-level checklists work with any `ChecklistScorer` configuration (`mode=\"batch\"`, `mode=\"item\"`, weighted, normalized) just like instance-level checklists."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
