{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoChecklist Pipeline Demo\n",
    "\n",
    "This notebook demonstrates the **Pipeline API** for easy checklist generation and scoring.\n",
    "\n",
    "The pipeline API provides:\n",
    "- **One-liner evaluation**: `pipeline(\"tick\")(input=\"...\", target=\"...\")`\n",
    "- **Custom prompts**: Load generator/scorer prompts from `.md` files\n",
    "- **Component registry**: Discover and instantiate components by name\n",
    "- **Batch evaluation**: Score entire datasets with progress bars\n",
    "- **Config-driven usage**: Create pipelines from configuration dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:01:14.565876Z",
     "iopub.status.busy": "2026-02-27T06:01:14.565741Z",
     "iopub.status.idle": "2026-02-27T06:01:14.572592Z",
     "shell.execute_reply": "2026-02-27T06:01:14.571976Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# OpenRouter is the default provider. Set OPENROUTER_API_KEY in .env\n",
    "# For other providers: OPENAI_API_KEY (OpenAI direct), or use vLLM (no key needed)\n",
    "assert os.getenv(\"OPENROUTER_API_KEY\"), \"Please set OPENROUTER_API_KEY in .env file\"\n",
    "\n",
    "MODEL = \"openai/gpt-5-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Component Registry\n",
    "\n",
    "The registry allows you to discover available components without knowing their class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:01:14.575039Z",
     "iopub.status.busy": "2026-02-27T06:01:14.574910Z",
     "iopub.status.idle": "2026-02-27T06:01:14.701102Z",
     "shell.execute_reply": "2026-02-27T06:01:14.700281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available generators: ['tick', 'rocketeval', 'rlcf_direct', 'rlcf_candidate', 'rlcf_candidates_only', 'feedback', 'checkeval', 'interacteval']\n",
      "Available scorers: ['batch', 'item', 'weighted', 'normalized']\n",
      "Available refiners: ['deduplicator', 'tagger', 'unit_tester', 'selector']\n"
     ]
    }
   ],
   "source": [
    "from autochecklist import list_generators, list_scorers, list_refiners\n",
    "\n",
    "print(\"Available generators:\", list_generators())\n",
    "print(\"Available scorers:\", list_scorers())\n",
    "print(\"Available refiners:\", list_refiners())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:01:14.746748Z",
     "iopub.status.busy": "2026-02-27T06:01:14.746539Z",
     "iopub.status.idle": "2026-02-27T06:01:14.754050Z",
     "shell.execute_reply": "2026-02-27T06:01:14.753424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  tick                   [instance] Few-shot checklist generation from input text\n",
      "                          └─ Uses a TICK-style prompt to generate yes/no checklist items from the input alone. No reference needed.\n",
      "                          scorer: batch\n",
      "\n",
      "  rocketeval             [instance] Confidence-aware checklist from input + reference\n",
      "                          └─ Generates checklist items informed by a reference response, scored via logprobs for calibrated confidence levels.\n",
      "                          scorer: normalized  [needs ref]\n",
      "\n",
      "  rlcf_direct            [instance] Weighted criteria from input + reference\n",
      "                          └─ Generates weighted checklist items (0-100 importance) by comparing the input against a reference response.\n",
      "                          scorer: weighted  [needs ref]\n",
      "\n",
      "  rlcf_candidate         [instance] Contrastive generation with candidates + reference\n",
      "                          └─ Auto-generates candidate responses, then creates weighted checklist items by contrasting candidates against the reference.\n",
      "                          scorer: weighted  [needs ref]\n",
      "\n",
      "  rlcf_candidates_only   [instance] Contrastive generation from candidates only\n",
      "                          └─ Auto-generates candidate responses and derives weighted checklist items by comparing candidates. No reference needed.\n",
      "                          scorer: weighted\n",
      "\n",
      "  feedback               [corpus  ] Generator that induces checklists from observations.\n",
      "                          └─ Takes a collection of evaluative observations (e.g., reviewer feedback, user complaints, quality notes, strengths/weaknesses) and generates a comprehensive yes/no checklist that addresses them. The pipeline applies multiple refinement steps: - Deduplication (merge similar questions) - Tagging (filter for applicability) - Selection (beam search for diverse subset)\n",
      "                          scorer: —\n",
      "\n",
      "  checkeval              [corpus  ] Generate checklists from evaluation dimension definitions.\n",
      "                          └─ CheckEval creates binary yes/no evaluation questions organized by dimension and sub-dimension. Questions can be provided as seeds or generated from dimension definitions.\n",
      "                          scorer: —\n",
      "\n",
      "  interacteval           [corpus  ] Generate checklists from interactive think-aloud attributes.\n",
      "                          └─ InteractEval takes pre-collected think-aloud attributes (considerations about evaluating a dimension) and transforms them through a 5-stage pipeline into a validated checklist of yes/no questions.\n",
      "                          scorer: —\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get detailed info for UI dropdowns\n",
    "from autochecklist.registry import list_generators_with_info\n",
    "\n",
    "for info in list_generators_with_info():\n",
    "    ref = \"  [needs ref]\" if info.get(\"requires_reference\") else \"\"\n",
    "    scorer = info.get(\"default_scorer\") or \"—\"\n",
    "    print(f\"  {info['name']:22} [{info['level']:8}] {info['description']}\")\n",
    "    if info.get(\"detail\"):\n",
    "        print(f\"  {'':22}  └─ {info['detail']}\")\n",
    "    print(f\"  {'':22}  scorer: {scorer}{ref}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Simple Pipeline - One-liner Evaluation\n",
    "\n",
    "The `pipeline()` function creates a ready-to-use pipeline (like HuggingFace's `transformers.pipeline`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:01:14.756025Z",
     "iopub.status.busy": "2026-02-27T06:01:14.755892Z",
     "iopub.status.idle": "2026-02-27T06:01:39.623314Z",
     "shell.execute_reply": "2026-02-27T06:01:39.621623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checklist (4 items):\n",
      "1. Is the response written as a three-line poem?\n",
      "2. Does the poem follow a 5-7-5 syllable structure across the three lines?\n",
      "3. Is the poem explicitly about autumn (mentions autumn or clearly references autumnal elements such as falling leaves, chill, harvest, migration)?\n",
      "4. Does the poem use concrete sensory imagery (sight, sound, smell, touch, or taste) to evoke the season?\n",
      "\n",
      "Pass rate: 100%\n"
     ]
    }
   ],
   "source": [
    "from autochecklist import pipeline\n",
    "\n",
    "# Create a TICK pipeline\n",
    "pipe = pipeline(\"tick\", generator_model=MODEL, scorer_model=MODEL)\n",
    "\n",
    "# Evaluate a response in one line\n",
    "result = pipe(\n",
    "    input=\"Write a haiku about autumn\",\n",
    "    target=\"Leaves fall gently down\\nCrisp air whispers through the trees\\nNature's final bow\"\n",
    ")\n",
    "\n",
    "print(f\"Checklist ({len(result.checklist)} items):\")\n",
    "print(result.checklist.to_text())\n",
    "print(f\"\\nPass rate: {result.pass_rate:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:01:39.626560Z",
     "iopub.status.busy": "2026-02-27T06:01:39.626184Z",
     "iopub.status.idle": "2026-02-27T06:01:52.018763Z",
     "shell.execute_reply": "2026-02-27T06:01:52.017179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated checklist (no scoring):\n",
      "1. Is the response written as a poem?\n",
      "2. Does the poem consist of five lines?\n",
      "3. Do lines 1, 2, and 5 rhyme with each other and lines 3 and 4 rhyme with each other (AABBA rhyme scheme)?\n",
      "4. Does the poem exhibit a limerick-like rhythm (longer meter in lines 1, 2, and 5 and shorter meter in lines 3 and 4)?\n",
      "5. Does the poem include a humorous, whimsical, or surprising twist typical of limericks?\n",
      "\n",
      "Score: None  # None since no response provided\n"
     ]
    }
   ],
   "source": [
    "# Generate-only mode (no scoring)\n",
    "result = pipe(input=\"Write a limerick\")\n",
    "\n",
    "print(\"Generated checklist (no scoring):\")\n",
    "print(result.checklist.to_text())\n",
    "print(f\"\\nScore: {result.score}  # None since no response provided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3. Pipeline with Custom Components\n\nFor more control, use `ChecklistPipeline` directly with custom components.\n\nYou can also load custom prompts from `.md` files by registering them first:\n```python\nfrom autochecklist import register_custom_generator, register_custom_scorer\n\n# Custom generator prompt\nregister_custom_generator(\"my_eval\", \"my_gen.md\")\npipe = pipeline(\"my_eval\", generator_model=MODEL, scorer_model=MODEL)\n\n# Custom scorer prompt (works with any generator)\nregister_custom_scorer(\"my_scorer\", \"my_scorer.md\")\npipe = pipeline(\"tick\", generator_model=MODEL, scorer=\"my_scorer\")\n\n# Custom scorer with config kwargs (mode, primary_metric, etc.)\nregister_custom_scorer(\"my_weighted\", \"my_scorer.md\", mode=\"item\", primary_metric=\"weighted\")\npipe = pipeline(\"tick\", generator_model=MODEL, scorer=\"my_weighted\")\n```\n\nOr register a full custom pipeline with scorer config in one call:\n```python\nfrom autochecklist import register_custom_pipeline\n\nregister_custom_pipeline(\n    \"my_eval\",\n    generator_prompt=\"Generate yes/no questions for:\\n\\n{input}\",\n    scorer_mode=\"item\",\n    primary_metric=\"weighted\",\n    scorer_prompt=\"rlcf\",  # built-in name, inline text, or Path to file\n)\npipe = pipeline(\"my_eval\", generator_model=MODEL, scorer_model=MODEL)\n```\n\nSee `custom_components_tutorial.ipynb` for details and full examples."
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:01:52.021968Z",
     "iopub.status.busy": "2026-02-27T06:01:52.021672Z",
     "iopub.status.idle": "2026-02-27T06:03:03.072708Z",
     "shell.execute_reply": "2026-02-27T06:03:03.071040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted score: 29%\n",
      "\n",
      "Checklist items with weights:\n",
      "  [100] Is the provided code valid Python syntax (i.e., can be parsed without syntax errors)?\n",
      "  [100] Does the code define a callable named 'factorial'?\n",
      "  [ 80] Does the 'factorial' function accept exactly one parameter?\n",
      "  [100] Does calling factorial(0) return 1?\n",
      "  [ 90] Does calling factorial(1) return 1?\n",
      "  [ 95] Does calling factorial(5) return 120?\n",
      "  [ 60] For a negative integer input (e.g., -1), does the function raise an error or otherwise explicitly refuse to compute a factorial rather than returning an incorrect numeric value?\n"
     ]
    }
   ],
   "source": [
    "from autochecklist import ChecklistPipeline, DirectGenerator, ChecklistScorer\n",
    "\n",
    "# Create pipeline with explicit components\n",
    "pipe = ChecklistPipeline(\n",
    "    generator=DirectGenerator(method_name=\"rlcf_direct\", model=MODEL),\n",
    "    scorer=ChecklistScorer(mode=\"item\", primary_metric=\"weighted\", model=MODEL),\n",
    ")\n",
    "\n",
    "# RLCF direct requires a reference response\n",
    "input_text = \"Write a factorial function in Python\"\n",
    "reference = \"\"\"def factorial(n):\n",
    "    if n < 0:\n",
    "        raise ValueError(\"n must be non-negative\")\n",
    "    return 1 if n <= 1 else n * factorial(n-1)\"\"\"\n",
    "\n",
    "result = pipe(\n",
    "    input=input_text,\n",
    "    reference=reference,\n",
    "    target=\"def fact(x): return x * fact(x-1) if x > 1 else 1\"\n",
    ")\n",
    "\n",
    "print(f\"Weighted score: {result.score.weighted_score:.0%}\")\n",
    "print(f\"\\nChecklist items with weights:\")\n",
    "for item in result.checklist.items:\n",
    "    print(f\"  [{item.weight:3.0f}] {item.question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Batch Corpus Evaluation\n",
    "\n",
    "Evaluate entire datasets efficiently with `run_batch()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:03:03.075881Z",
     "iopub.status.busy": "2026-02-27T06:03:03.075595Z",
     "iopub.status.idle": "2026-02-27T06:04:32.231008Z",
     "shell.execute_reply": "2026-02-27T06:04:32.229449Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Scoring:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating:  33%|███▎      | 1/3 [00:19<00:38, 19.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Scoring:  33%|███▎      | 1/3 [00:29<00:58, 29.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating:  67%|██████▋   | 2/3 [00:45<00:23, 23.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Scoring:  67%|██████▋   | 2/3 [00:56<00:28, 28.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating: 100%|██████████| 3/3 [01:09<00:00, 23.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Scoring: 100%|██████████| 3/3 [01:29<00:00, 30.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating: 100%|██████████| 3/3 [01:29<00:00, 29.71s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "Scoring: 100%|██████████| 3/3 [01:29<00:00, 29.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Macro-average pass rate: 75%\n",
      "  Micro-average pass rate (i.e., DRFR): 77%\n",
      "\n",
      "Per-item scores:\n",
      "  1. Write a haiku about spring     -> 100%\n",
      "  2. Write a haiku about summer     -> 100%\n",
      "  3. Write a haiku about winter     -> 25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare a small dataset\n",
    "data = [\n",
    "    {\n",
    "        \"input\": \"Write a haiku about spring\",\n",
    "        \"target\": \"Cherry blossoms bloom\\nSoft petals drift on the breeze\\nNew life awakens\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Write a haiku about summer\",\n",
    "        \"target\": \"Golden sun beats down\\nCicadas sing in the heat\\nLazy days drift by\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Write a haiku about winter\", \n",
    "        \"target\": \"I like winter because it snows and I can make snowmen.\"  # Not a haiku!\n",
    "    },\n",
    "]\n",
    "\n",
    "pipe = pipeline(\"tick\", generator_model=MODEL, scorer_model=MODEL)\n",
    "\n",
    "# Run batch evaluation (generates one checklist per instruction)\n",
    "result = pipe.run_batch(data, show_progress=True)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Macro-average pass rate: {result.macro_pass_rate:.0%}\")\n",
    "print(f\"  Micro-average pass rate (i.e., DRFR): {result.micro_pass_rate:.0%}\")\n",
    "print(f\"\\nPer-item scores:\")\n",
    "for i, (item, score) in enumerate(zip(data, result.scores)):\n",
    "    print(f\"  {i+1}. {item['input'][:30]:30} -> {score.pass_rate:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:04:32.233476Z",
     "iopub.status.busy": "2026-02-27T06:04:32.233189Z",
     "iopub.status.idle": "2026-02-27T06:05:17.733418Z",
     "shell.execute_reply": "2026-02-27T06:05:17.732032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared checklist:\n",
      "1. Is the response written in haiku form (three short lines)?\n",
      "2. Does the haiku adhere to a 5-7-5 syllable structure?\n",
      "3. Is the haiku explicitly about a season (mentions a season or uses clear seasonal imagery)?\n",
      "4. Does the haiku use concise, sensory imagery rather than exposition or explanation?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With shared checklist:\n",
      "  Macro-average pass rate: 75%\n"
     ]
    }
   ],
   "source": [
    "# Use a shared checklist for all evaluations\n",
    "# Useful when you want to apply the same criteria to all responses\n",
    "\n",
    "# First generate a shared checklist\n",
    "shared_checklist = pipe.generate(input=\"Write a haiku about any season\")\n",
    "print(\"Shared checklist:\")\n",
    "print(shared_checklist.to_text())\n",
    "\n",
    "# Then evaluate all responses against it\n",
    "result = pipe.run_batch(data, checklist=shared_checklist)\n",
    "\n",
    "print(f\"\\nWith shared checklist:\")\n",
    "print(f\"  Macro-average pass rate: {result.macro_pass_rate:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:05:17.737189Z",
     "iopub.status.busy": "2026-02-27T06:05:17.736649Z",
     "iopub.status.idle": "2026-02-27T06:05:18.215985Z",
     "shell.execute_reply": "2026-02-27T06:05:18.214096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to evaluation_results.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": \"Write a haiku about spring\", \"target\": \"Cherry blossoms bloom\\nSoft petals drift on the breeze\\nNew life awakens\", \"pass_rate\": 1.0, \"item_scores\": [{\"item_id\": \"73e81fe4\", \"answer\": \"yes\", \"reasoning\": null}, {\"item_id\": \"14f4e2b8\", \"answer\": \"yes\", \"reasoning\": null}, {\"item_id\": \"b06fde4e\", \"answer\": \"yes\", \"reasoning\": null}, {\"item_id\": \"1f817be1\", \"answer\": \"yes\", \"reasoning\": null}], \"weighted_score\": 1.0, \"normalized_score\": 1.0}\r\n",
      "{\"input\": \"Write a haiku about summer\", \"target\": \"Golden sun beats down\\nCicadas sing in the heat\\nLazy days drift by\", \"pass_rate\": 1.0, \"item_scores\": [{\"item_id\": \"73e81fe4\", \"answer\": \"yes\", \"reasoning\": null}, {\"item_id\": \"14f4e2b8\", \"answer\": \"yes\", \"reasoning\": null}, {\"item_id\": \"b06fde4e\", \"answer\": \"yes\", \"reasoning\": null}, {\"item_id\": \"1f817be1\", \"answer\": \"yes\", \"reasoning\": null}], \"weighted_score\": 1.0, \"normalized_score\": 1.0}\r\n"
     ]
    }
   ],
   "source": [
    "# Export to JSONL\n",
    "result.to_jsonl(\"evaluation_results.jsonl\")\n",
    "print(\"Saved to evaluation_results.jsonl\")\n",
    "\n",
    "# View the file\n",
    "!head -2 evaluation_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:05:18.220073Z",
     "iopub.status.busy": "2026-02-27T06:05:18.219740Z",
     "iopub.status.idle": "2026-02-27T06:05:18.569466Z",
     "shell.execute_reply": "2026-02-27T06:05:18.568239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>pass_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Write a haiku about spring</td>\n",
       "      <td>Cherry blossoms bloom\\nSoft petals drift on th...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Write a haiku about summer</td>\n",
       "      <td>Golden sun beats down\\nCicadas sing in the hea...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Write a haiku about winter</td>\n",
       "      <td>I like winter because it snows and I can make ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        input  \\\n",
       "0  Write a haiku about spring   \n",
       "1  Write a haiku about summer   \n",
       "2  Write a haiku about winter   \n",
       "\n",
       "                                              target  pass_rate  \n",
       "0  Cherry blossoms bloom\\nSoft petals drift on th...       1.00  \n",
       "1  Golden sun beats down\\nCicadas sing in the hea...       1.00  \n",
       "2  I like winter because it snows and I can make ...       0.25  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Export to pandas DataFrame (requires pandas)\n",
    "try:\n",
    "    df = result.to_dataframe()\n",
    "    display(df[[\"input\", \"target\", \"pass_rate\"]])\n",
    "except ImportError:\n",
    "    print(\"Install pandas for DataFrame export: pip install pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:05:18.572047Z",
     "iopub.status.busy": "2026-02-27T06:05:18.571883Z",
     "iopub.status.idle": "2026-02-27T06:05:18.576828Z",
     "shell.execute_reply": "2026-02-27T06:05:18.576000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checklist to checklist.json\n",
      "Loaded checklist with 4 items:\n",
      "1. Is the response written in haiku form (three short lines)?\n",
      "2. Does the haiku adhere to a 5-7-5 syllable structure?\n",
      "3. Is the haiku explicitly about a season (mentions a season or uses clear seasonal imagery)?\n",
      "4. Does the haiku use concise, sensory imagery rather than exposition or explanation?\n"
     ]
    }
   ],
   "source": [
    "# Save and load checklists (Pydantic models)\n",
    "from autochecklist import Checklist\n",
    "\n",
    "# Save checklist to JSON file\n",
    "with open(\"checklist.json\", \"w\") as f:\n",
    "    f.write(shared_checklist.model_dump_json(indent=2))\n",
    "\n",
    "print(\"Saved checklist to checklist.json\")\n",
    "\n",
    "# Load checklist from JSON file\n",
    "with open(\"checklist.json\") as f:\n",
    "    loaded_checklist = Checklist.model_validate_json(f.read())\n",
    "\n",
    "print(f\"Loaded checklist with {len(loaded_checklist)} items:\")\n",
    "print(loaded_checklist.to_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Config-Driven Instantiation\n",
    "\n",
    "Create pipelines from configuration dicts (useful for UI integration or config files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:05:18.579478Z",
     "iopub.status.busy": "2026-02-27T06:05:18.579340Z",
     "iopub.status.idle": "2026-02-27T06:05:38.085765Z",
     "shell.execute_reply": "2026-02-27T06:05:38.084834Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4034641/383983590.py:15: DeprecationWarning: BatchScorer is deprecated, use ChecklistScorer(mode='batch')\n",
      "  scorer = scorer_cls(model=config[\"model\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: tick\n",
      "Scorer: batch\n",
      "Pass rate: 100%\n"
     ]
    }
   ],
   "source": [
    "from autochecklist import get_generator, get_scorer\n",
    "\n",
    "# Configuration dict (could come from a config file or UI)\n",
    "config = {\n",
    "    \"generator\": \"tick\",\n",
    "    \"scorer\": \"batch\",  # or \"item\" for one-question-per-call\n",
    "    \"model\": MODEL,\n",
    "}\n",
    "\n",
    "# Create components from config\n",
    "gen_cls = get_generator(config[\"generator\"])\n",
    "scorer_cls = get_scorer(config[\"scorer\"])\n",
    "\n",
    "generator = gen_cls(model=config[\"model\"])\n",
    "scorer = scorer_cls(model=config[\"model\"])\n",
    "\n",
    "# Use them\n",
    "checklist = generator.generate(input=\"Say hello\")\n",
    "score = scorer.score(checklist, target=\"Hello there!\")\n",
    "\n",
    "print(f\"Generator: {generator.method_name}\")\n",
    "print(f\"Scorer: {scorer.scoring_method}\")\n",
    "print(f\"Pass rate: {score.pass_rate:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Summary\n\n| Feature | Description |\n|---------|-------------|\n| `pipeline(\"task\")` | Quick pipeline creation (HuggingFace-style) |\n| `register_custom_generator(\"name\", \"file.md\")` | Register a custom `.md` prompt for generation |\n| `register_custom_scorer(\"name\", \"file.md\", mode=..., primary_metric=...)` | Register a custom scorer with config |\n| `register_custom_pipeline(\"name\", generator_prompt=..., scorer_mode=..., primary_metric=...)` | Register a full custom pipeline preset |\n| `save_pipeline_config(\"name\", \"file.json\")` / `load_pipeline_config(\"file.json\")` | Save/load pipeline configs as JSON |\n| `pipeline(\"name\", scorer=\"scorer_name\")` | Use registered custom generator + scorer |\n| `ChecklistPipeline` | Full control with custom components |\n| `list_generators()` | Discover available generators |\n| `get_generator(name)` | Get generator class by name |\n| `run_batch(data)` | Batch evaluation with metrics |\n| `to_dataframe()` | Export results to pandas |\n| `to_jsonl(path)` | Export results to JSONL |\n| `checklist.model_dump_json()` | Save checklist to JSON |\n| `Checklist.model_validate_json()` | Load checklist from JSON |\n\nSee `custom_components_tutorial.ipynb` for a full walkthrough of custom `.md` prompts, registration, and advanced subclassing."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Multi-Provider Support\n",
    "\n",
    "The pipeline API supports multiple LLM providers. All components accept `provider`, `base_url`, `client`, and `api_format` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T06:05:38.087867Z",
     "iopub.status.busy": "2026-02-27T06:05:38.087648Z",
     "iopub.status.idle": "2026-02-27T06:05:38.095035Z",
     "shell.execute_reply": "2026-02-27T06:05:38.094328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator provider: openai\n",
      "Scorer provider: openai\n"
     ]
    }
   ],
   "source": [
    "from autochecklist import pipeline, VLLMOfflineClient\n",
    "\n",
    "# OpenRouter (default)\n",
    "pipe = pipeline(\"tick\", generator_model=\"openai/gpt-4o-mini\", scorer_model=\"openai/gpt-4o-mini\")\n",
    "\n",
    "# OpenAI direct — lower latency, better caching\n",
    "pipe = pipeline(\"tick\", provider=\"openai\", generator_model=\"gpt-4o-mini\")\n",
    "\n",
    "# vLLM server mode — self-hosted, OpenAI-compatible\n",
    "pipe = pipeline(\"tick\", provider=\"vllm\", base_url=\"http://gpu-server:8000/v1\")\n",
    "\n",
    "# vLLM offline — direct Python inference, no server needed\n",
    "# client = VLLMOfflineClient(model=\"google/gemma-3-1b-it\")\n",
    "# pipe = pipeline(\"tick\", client=client)\n",
    "\n",
    "# OpenAI Responses API — opt-in, better caching on OpenAI's side\n",
    "pipe = pipeline(\"tick\", provider=\"openai\", generator_model=\"gpt-4o-mini\", api_format=\"responses\")\n",
    "\n",
    "# Provider params propagate to all sub-components (generator + scorer)\n",
    "pipe = pipeline(\"tick\", provider=\"openai\", generator_model=\"gpt-4o-mini\")\n",
    "print(f\"Generator provider: {pipe.generator._provider}\")\n",
    "print(f\"Scorer provider: {pipe.scorer._provider}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}